[
  {
    "objectID": "01-linear-regression.html",
    "href": "01-linear-regression.html",
    "title": "Regressão Linear",
    "section": "",
    "text": "import numpy as np\n\nEm primeiro lugar simulamos alguns dados. x vai ser uma variável contendo n valores aleatórios obtidos a partir da distribuição uniforme.\n\nn = 1000\nx = np.random.uniform(size=(n,))\n\nEm seguida definimos os pesos W e b para a simulação. Nosso objetivo depois, vai ser fingir que não sabemos esses valores e ajustar um modelo para encontrá-los.\n\nW = 0.9\nb = 0.1\n\nDefinimos então a variável resposta:\n\ny = W * x + b\n\nAgora vamos definir o nosso modelo. Ele é uma classe do python que recebe os valores iniciais dos pesos w e b na inicialização e possui o método predict que faz uma transformação linear w*x + b no input x.\n\nclass Model:\n    def __init__(self, w, b):\n        self.w = w\n        self.b = b\n\n    def predict(self, x):\n        return self.w * x + self.b\n\nAgora vamos definir a função de perda. Nesse caso estamos usando o erro quadrático médio:\n\ndef loss(y_true, y_hat):\n    return np.mean((y_true - y_hat) ** 2)\n\nTambém definimos as derivadas da função de perda l em relação a cada uma das partes do grafo de computação até chegar nos parâmetros que queremos estimar.\n\n# derivada da função de perda em relação à y_hat\ndef dl_dyhat(y_hat, x, y):\n    return 2 * (y - y_hat) * (-1)\n\n# derivada de y_hat com relação a w\ndef dyhat_dw(y_hat, x, y):\n    return x\n\n# derivada de y_hat com relação a b (viés)\ndef dyhat_db(y_hat, x, y):\n    return 1.0\n\nAgora vamos inicializar o modelo:\n\nw = np.random.uniform(size=1)\nb = 0.0\nmodel = Model(w, b)\n\nE podemos, finalmente, implementar o método da descida do gradiente.\n\nlr = 0.1 # learning rate - qual o tamanho do passo que damos em cada iteração.\nfor step in range(500):\n    y_hat = model.predict(x)\n    dldyhat = dl_dyhat(y_hat, x, y)\n    model.w -= lr * np.mean(dldyhat * dyhat_dw(y_hat, x, y))\n    model.b -= lr * np.mean(dldyhat * dyhat_db(y_hat, x, y))\n    if (step % 100) == 0:\n        print(\"step:\", step, \"loss:\", loss(y, y_hat))\n\nstep: 0 loss: 0.22998169398463467\nstep: 100 loss: 0.001345444126299616\nstep: 200 loss: 9.543931127460992e-05\nstep: 300 loss: 6.770004014676807e-06\nstep: 400 loss: 4.802314030416836e-07\n\n\nVerificamos que o resultado está conforme o esperado. Isto é, w e b estão parecidos com os valores usamos para gerar os dados.\n\nprint(model.w)\nprint(model.b)\n\n[0.89936771]\n0.10034437382944898"
  },
  {
    "objectID": "02-sgd.html",
    "href": "02-sgd.html",
    "title": "Stochastic gradient descent",
    "section": "",
    "text": "O objetivo desse exemplo é simular alguns dados e ajustar um modelo de regressão linear usando o método estocástico da descida do gradiente. Importante notar que queremos implementar o método da descida do gradiente de forma manual, isto é, sem usar bibliotecas que calculam a derivada automaticamente.\n\n\n\n\n\n\nNote\n\n\n\nA vantagem do SGD com relação ao GD é o custom computacional. O primeiro precisa calcular derivadas de todas as observações, calcular o valor predito para todas as observações, etc para cada passo. No SGD, como fazemos uma observação de cada vez, chegamos mais rápido no mínimo.\nOutra vantagem é o uso da memória. Em deep learning muitas vezes não conseguimos carregar todos os dados de uma vez para a memória do computador. No caso do SGD, é trivial implementá-lo sem precisar de todas as observações de uma vez.\nAlém disso, é estudado que o SGD tem um efeito de regularização no ajuste do modelo.\n\n\n\nimport numpy as np\n\nEm primeiro lugar simulamos alguns dados. x vai ser uma variável contendo n valores aleatórios obtidos a partir da distribuição uniforme.\n\nn = 1000\nx = np.random.uniform(size=(n,))\n\nEm seguida definimos os pesos W e b para a simulação. Nosso objetivo depois, vai ser fingir que não sabemos esses valores e ajustar um modelo para encontrá-los.\n\nW = 0.9\nb = 0.1\n\nDefinimos então a variável resposta:\n\ny = W * x + b\n\nAgora vamos definir o nosso modelo. Ele é uma classe do python que recebe os valores iniciais dos pesos w e b na inicialização e possui o método predict que faz uma transformação linear w*x + b no input x.\n\nclass Model:\n    def __init__(self, w, b):\n        self.w = w\n        self.b = b\n\n    def predict(self, x):\n        return self.w * x + self.b\n\nAgora vamos definir a função de perda. Nesse caso estamos usando o erro quadrático médio:\n\ndef loss(y_true, y_hat):\n    return np.mean((y_true - y_hat) ** 2)\n\nTambém definimos as derivadas da função de perda l em relação a cada uma das partes do grafo de computação até chegar nos parâmetros que queremos estimar.\n\n# derivada da função de perda em relação à y_hat\ndef dl_dyhat(y_hat, x, y):\n    return 2 * (y - y_hat) * (-1)\n\n# derivada de y_hat com relação a w\ndef dyhat_dw(y_hat, x, y):\n    return x\n\n# derivada de y_hat com relação a b (viés)\ndef dyhat_db(y_hat, x, y):\n    return 1.0\n\nAgora vamos inicializar o modelo:\n\nw = np.random.uniform(size=1)\nb = 0.0\nmodel = Model(w, b)\n\nE podemos, finalmente, implementar o método da descida do gradiente.\n\nlr = 0.1 # learning rate - qual o tamanho do passo que damos em cada iteração.\nnum_epochs = 2 # o número de épocas é o número de vezes que passamos pela base inteira.\nindex = np.arange(x.size)\n\nfor epoch in range(num_epochs):\n    np.random.shuffle(index)\n    for i in list(index):\n        y_hat = model.predict(x[i])\n        model.w -= lr * np.mean(\n            dl_dyhat(y_hat, x[i], y[i]) * dyhat_dw(y_hat, x[i], y[i])\n        )\n        model.b -= lr * np.mean(\n            dl_dyhat(y_hat, x[i], y[i]) * dyhat_db(y_hat, x[i], y[i])\n        )\n    print(\"epoch:\", epoch, \"loss:\", loss(y, model.predict(x)))\n\nepoch: 0 loss: 4.285631346360619e-15\nepoch: 1 loss: 3.5306078809548166e-27\n\n\nVerificamos que o resultado está conforme o esperado. Isto é, w e b estão parecidos com os valores usamos para gerar os dados.\n\nprint(model.w)\nprint(model.b)\n\n[0.9]\n0.10000000000011379"
  },
  {
    "objectID": "03-keras.html",
    "href": "03-keras.html",
    "title": "Keras",
    "section": "",
    "text": "Note\n\n\n\nO Keras é uma biblioteca que faz parte do TensorFlow e possui uma sintaxe concisa para definir modelos de deep learning. O TensorFlow além de ser rápido e possuir implementações para GPU’s, possui uma feature chamada automatic differentiation que calcular as derivadas automaticamente.\n\n\nOs dados estão sendo gerados da mesma forma que no exemplo 1.\n\nimport numpy as np\n\n# Data generation ----------------------------------------------\n\nn = 1000\nx = np.random.uniform(size=(n,))\n\nW = 0.9\nb = 0.1\n\ny = W * x + b\n\nVamos importar o Keras.\n\nfrom tensorflow import keras\n\nAgora vamos definir o modelo no Keras usando a API funcional: Para definir modelos usando essa API fazemos:\n\nDefinimos o formato do nosso input usando a camada keras.layers.Input. Nesse passo omitimos o número de observações. O (1,) nesse exemplo significa que o nosso input é uma matriz com 1 coluna.\nDefinimos as transformações que vamos fazer no nosso input. Nesse caso usamos a camada keras.layers.Dense que faz a transformação linear W*x+ b.\nPor fim definimos um modelo do Keras especificando um input e um output.\n\n\ninput = keras.layers.Input(shape=(1,))\noutput = keras.layers.Dense(units=1, use_bias=True)(input)\nmodel = keras.Model(inputs=input, outputs=output)\n\nEsse modelo possui 2 parâmetros: w e b.\n\nmodel.summary()\n\nModel: \"model_3\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n input_4 (InputLayer)        [(None, 1)]               0         \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 2         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 2\n\n\nTrainable params: 2\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\nEm seguida compilamos o modelo falando qual é a função de perda que queremos minimizar e qual o método de otimização.\n\nmodel.compile(\n    loss=keras.losses.mean_squared_error,\n    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n)\n\nPodemos então ajustar o modelo passando para o método fit os dados de input e output. Também especificamos um batch_size=1 (para ficar igual ao SGD teórico) e o número de epochs. Só estamos usando verbose=2 para a barra de prorgresso não aparecer neste site, você pode ignorar esse parâmetro.\n\nhistory = model.fit(x=x, y=y, batch_size=1, epochs=5, verbose=2)\n\nEpoch 1/5\n\n\n1000/1000 - 0s - loss: 0.0015 - 489ms/epoch - 489us/step\n\n\nEpoch 2/5\n\n\n1000/1000 - 0s - loss: 3.8309e-05 - 382ms/epoch - 382us/step\n\n\nEpoch 3/5\n\n\n1000/1000 - 0s - loss: 2.5557e-06 - 363ms/epoch - 363us/step\n\n\nEpoch 4/5\n\n\n1000/1000 - 0s - loss: 1.7280e-07 - 361ms/epoch - 361us/step\n\n\nEpoch 5/5\n\n\n1000/1000 - 0s - loss: 1.1419e-08 - 365ms/epoch - 365us/step\n\n\nPodemos em seguida visualizar o valor da função de perda ao longo do número de épocas:\n\nimport seaborn as sns\nsns.lineplot(x=range(5), y=history.history[\"loss\"])\n\n<AxesSubplot:>\n\n\n\n\n\nVisualização do grafo do modelo:\n\nkeras.utils.plot_model(model, show_shapes=True)\n\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n\n\nVerificar que os pesos obtidos são o mesmo que usamos apra simular os dados:\n\nmodel.get_weights()\n\n[array([[0.89984226]], dtype=float32), array([0.10008816], dtype=float32)]\n\n\nFazer previsões com o modelo ajustado:\n\nmodel.predict(x)[0:5]\n\narray([[0.8098919 ],\n       [0.66679585],\n       [0.49405667],\n       [0.81630385],\n       [0.78361654]], dtype=float32)"
  },
  {
    "objectID": "04-mlp.html",
    "href": "04-mlp.html",
    "title": "Multi-Layer Perceptron",
    "section": "",
    "text": "O Keras já usa a notação matricial, por isso, podemos ver que o código no Keras muda bem pouco.\n\nimport numpy as np\nfrom tensorflow import keras\n\nVeja que nesse exemplo a geração dos dados é um pouco mais complicada. Usamos a mesma idéia do MLP.\n\nn = 1000\n\n# uma matriz com `n` observações e 2 colunas. \n# valores aleatórios obtidos da distribuição uniforme.\nX = np.random.uniform(size=(n, 2))\n\n# função relu, usada comunmente como função de ativação em \n# redes neurais.\ndef relu(x):\n    return x * (x > 0)\n\n# Dois conjuntos de pesos: W e B para a primeira camada, V e a para a segunda.\nW = np.array([[0.2, 0.7], [-0.4, 0.5]])\nB = np.array([0.1, -0.2])\nV = np.array([[0.6], [0.7]])\na = 0.9\n\n# O y é gerado dessa forma\n# `np.dot()` é a mesma coisa que multiplição de matrizes.\ny = relu(np.dot(X, W) + B)\ny = np.dot(y, V) + a\n\nAgora que os dados foram gerados vamos definir o modelo. Pontos importantes:\n\nA camada de input agora tem shape (2,), isto é o input é uma matriz com n linhas e 2 colunas.\nA camada escondida (hidden layer) do modelo possui 1000 unidades (ou neurônios). A função de ativação é relu. É muito importante que as camadas intermediárias do MLP possuam uma função de ativação.\n\n\ninput = keras.layers.Input(shape=(2,))\nhidden = keras.layers.Dense(units=1000, activation=\"relu\")(input)\noutput = keras.layers.Dense(units=1)(hidden)\n\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n\nModel: \"model_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n input_2 (InputLayer)        [(None, 2)]               0         \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 1000)              3000      \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 1001      \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 4,001\n\n\nTrainable params: 4,001\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\nA quantidade de parâmetros da primeira camada é dada por:\n\n2 parâmetros para cada neurônio (um por coluna do input) = 2000\n1 viés para cada neurônio = 1000\nTotal: 3000\n\nPara a camada de output temos fazemos:\n\n1000 parâmetros para cada neurônio = 1000\n1 viés para cada neurônios = 1\nTotal = 1001\n\nAgora da mesma forma que nos exemplos anteriores definimos a loss e o otimizador. Veja que o Keras também permite usar strings para definir esses atributos - 'mse' é um atalho para keras.losses.mean_squared_error.\n\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\n\nFinalmente podemos ajustar o modelo:\n\nhistory = model.fit(x=X, y=y, batch_size=10, epochs=5, verbose=2)\n\nEpoch 1/5\n\n\n100/100 - 0s - loss: 0.1332 - 206ms/epoch - 2ms/step\n\n\nEpoch 2/5\n\n\n100/100 - 0s - loss: 0.0028 - 55ms/epoch - 549us/step\n\n\nEpoch 3/5\n\n\n100/100 - 0s - loss: 0.0018 - 52ms/epoch - 519us/step\n\n\nEpoch 4/5\n\n\n100/100 - 0s - loss: 0.0013 - 53ms/epoch - 530us/step\n\n\nEpoch 5/5\n\n\n100/100 - 0s - loss: 0.0010 - 52ms/epoch - 523us/step"
  },
  {
    "objectID": "05-logistic-regression.html",
    "href": "05-logistic-regression.html",
    "title": "Regressão Logística",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\nfrom tensorflow import keras\nfrom sklearn import metrics\n\nVamos gerar os dados de input:\n\nn = 1000\n\nX = np.random.uniform(size=(n, 2))\nW = np.array([[-0.6], [0.7]])\nB = 0.1\n\nAgora vamos gerar a variável resposta. Note que primeiro geramos a probabilidade do y ser 1, usando sigmoid(np.dot(X, W) + B). Em seguida, se a probabilidade de y ser igual 1 for maior do que 0.5, dizemos que ele vale 1.\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ny = sigmoid(np.dot(X, W) + B)\nprint(\"Prob y=1\", y[0:5])\ny = 1.0 * (y > 0.5)\nprint(\"Y=\", y[0:5])\n\nProb y=1 [[0.44132695]\n [0.50520231]\n [0.54533653]\n [0.56898554]\n [0.61125024]]\nY= [[0.]\n [1.]\n [1.]\n [1.]\n [1.]]\n\n\nAgora definimos o modelo. A definição é exatamente igual a definição do modelo da regressão linear (exemplo 3) exceto pela funçõ de ativação sigmoid.\nA função de ativação sigmoid faz com que output de np.dot(X, W) + B que acontece dentro da camada densa seja um número entre 0 e 1.\n\ninput = keras.layers.Input(shape=(2,))\noutput = keras.layers.Dense(units=1, activation=\"sigmoid\")(input)\nmodel = keras.Model(inputs=input, outputs=output)\n\nmodel.summary()\n\nModel: \"model\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n input_1 (InputLayer)        [(None, 2)]               0         \n\n\n                                                                 \n\n\n dense (Dense)               (None, 1)                 3         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 3\n\n\nTrainable params: 3\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n2022-04-27 15:58:38.446237: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nAgora vamos compilar o modelo passando a função de perda, otimizador e métricas. Pontos importantes:\n\nComo o problema agora é de classificação binária (o y pode ter valores 0 ou 1), usamos a função de perda binary_crossentropy.\nPodemos passar uma lista de métricas para o Keras calcular durante o ajuste do modelo. Por exemplo aqui, pedimos para ele calcular a acurácia.\n\n\nmodel.compile(\n    loss=keras.losses.binary_crossentropy,\n    optimizer=keras.optimizers.SGD(learning_rate=0.1),\n    metrics=[\"accuracy\"]\n)\n\nAgora podemos ajustar o modelo. O parâmetro validation_split=0.1 fala para o Keras separar 10% das observações de (x,y) e usá-las apenas para reportar métricas nessa base de validação (as observações são selecionadas de forma aleatória).\n\nhistory = model.fit(x=X, y=y, batch_size=32, epochs=20, validation_split=0.1,\n                    verbose=2)\n\nEpoch 1/20\n\n\n29/29 - 0s - loss: 0.5566 - accuracy: 0.8033 - val_loss: 0.4929 - val_accuracy: 0.7700 - 459ms/epoch - 16ms/step\n\n\nEpoch 2/20\n\n\n29/29 - 0s - loss: 0.5110 - accuracy: 0.7011 - val_loss: 0.4615 - val_accuracy: 0.7700 - 46ms/epoch - 2ms/step\n\n\nEpoch 3/20\n\n\n29/29 - 0s - loss: 0.4896 - accuracy: 0.7122 - val_loss: 0.4423 - val_accuracy: 0.7800 - 38ms/epoch - 1ms/step\n\n\nEpoch 4/20\n\n\n29/29 - 0s - loss: 0.4717 - accuracy: 0.7233 - val_loss: 0.4272 - val_accuracy: 0.8100 - 45ms/epoch - 2ms/step\n\n\nEpoch 5/20\n\n\n29/29 - 0s - loss: 0.4559 - accuracy: 0.7500 - val_loss: 0.4111 - val_accuracy: 0.8200 - 42ms/epoch - 1ms/step\n\n\nEpoch 6/20\n\n\n29/29 - 0s - loss: 0.4413 - accuracy: 0.7700 - val_loss: 0.3972 - val_accuracy: 0.8400 - 38ms/epoch - 1ms/step\n\n\nEpoch 7/20\n\n\n29/29 - 0s - loss: 0.4279 - accuracy: 0.7844 - val_loss: 0.3854 - val_accuracy: 0.8400 - 38ms/epoch - 1ms/step\n\n\nEpoch 8/20\n\n\n29/29 - 0s - loss: 0.4159 - accuracy: 0.8067 - val_loss: 0.3746 - val_accuracy: 0.8700 - 36ms/epoch - 1ms/step\n\n\nEpoch 9/20\n\n\n29/29 - 0s - loss: 0.4048 - accuracy: 0.8222 - val_loss: 0.3633 - val_accuracy: 0.8800 - 37ms/epoch - 1ms/step\n\n\nEpoch 10/20\n\n\n29/29 - 0s - loss: 0.3946 - accuracy: 0.8344 - val_loss: 0.3539 - val_accuracy: 0.8800 - 37ms/epoch - 1ms/step\n\n\nEpoch 11/20\n\n\n29/29 - 0s - loss: 0.3851 - accuracy: 0.8444 - val_loss: 0.3443 - val_accuracy: 0.8800 - 37ms/epoch - 1ms/step\n\n\nEpoch 12/20\n\n\n29/29 - 0s - loss: 0.3762 - accuracy: 0.8444 - val_loss: 0.3364 - val_accuracy: 0.8900 - 36ms/epoch - 1ms/step\n\n\nEpoch 13/20\n\n\n29/29 - 0s - loss: 0.3679 - accuracy: 0.8567 - val_loss: 0.3293 - val_accuracy: 0.9000 - 37ms/epoch - 1ms/step\n\n\nEpoch 14/20\n\n\n29/29 - 0s - loss: 0.3599 - accuracy: 0.8678 - val_loss: 0.3237 - val_accuracy: 0.9200 - 37ms/epoch - 1ms/step\n\n\nEpoch 15/20\n\n\n29/29 - 0s - loss: 0.3527 - accuracy: 0.8822 - val_loss: 0.3170 - val_accuracy: 0.9200 - 37ms/epoch - 1ms/step\n\n\nEpoch 16/20\n\n\n29/29 - 0s - loss: 0.3457 - accuracy: 0.8833 - val_loss: 0.3108 - val_accuracy: 0.9300 - 38ms/epoch - 1ms/step\n\n\nEpoch 17/20\n\n\n29/29 - 0s - loss: 0.3393 - accuracy: 0.8889 - val_loss: 0.3046 - val_accuracy: 0.9300 - 37ms/epoch - 1ms/step\n\n\nEpoch 18/20\n\n\n29/29 - 0s - loss: 0.3331 - accuracy: 0.9044 - val_loss: 0.2972 - val_accuracy: 0.9300 - 37ms/epoch - 1ms/step\n\n\nEpoch 19/20\n\n\n29/29 - 0s - loss: 0.3274 - accuracy: 0.8967 - val_loss: 0.2937 - val_accuracy: 0.9600 - 37ms/epoch - 1ms/step\n\n\nEpoch 20/20\n\n\n29/29 - 0s - loss: 0.3219 - accuracy: 0.9089 - val_loss: 0.2866 - val_accuracy: 0.9500 - 39ms/epoch - 1ms/step\n\n\nPodemos fazer graáfio da acurácia ao longo das épocas:\n\nsns.lineplot(x=range(20), y=history.history[\"val_accuracy\"])\n\n<AxesSubplot:>\n\n\n\n\n\nE uma matriz de confusão para o ponto de corte 0.5 - isto é, se a probabilidade predita for maior que 0.5 classificamos como y=1\n\nmetrics.confusion_matrix(y, model.predict(X) > 0.5)\n\narray([[199,  94],\n       [  0, 707]])"
  },
  {
    "objectID": "06-convolution.html",
    "href": "06-convolution.html",
    "title": "Convolução",
    "section": "",
    "text": "from tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nComeçamos pegando uma imagem do banco de dados MNIST - famoso banco de dados de exemplos de deep learning que contém imagens de dígitos de 0 a 9 escritos à mão.\n\n(x_train, _), (_, _) = keras.datasets.mnist.load_data()\n\nPodemos visualizar algumas imagens com:\n\nplt.imshow(x_train[0])\nplt.show()\nplt.imshow(x_train[1])\nplt.show()\nplt.imshow(x_train[2])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCada imagem é representada por uma matrix 28x28 em que os valores são a intensidade de cor. Os valores vão de 0 a 255.\nAgora vamos definir um kernel de convolução de tamanho 3x3.\n\nw = np.random.uniform(size=(3, 3))\nw = np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]])\n\nLembre-se que na convolução vamos, para cada pixel da imagem, vamos posicionar esse kernel e fazer a multiplicação entre ele e os vizinhos para poder somar.\nPrimeiro vamos definir uma forma de encontrar os vizinhos. Dado um pixel na linha 9 e coluna doze da imagem, podemos obter os vizinhos da seguinte forma:\n\nimg = x_train[0, :, :] / 255.0\n# vizinhança de tamanho 3x3\ni = 9\nj = 12\n\nvizinhos = img[(i - 1) : (i + 2), (j - 1) : (j + 2)]\n\nAgora como sabemos como encontrar os vizinhos, vamos passear por todos os vizinhos para aplicar o algoritmo da convolução:\n\nnew_img = img\nfor i in range(1, img.shape[0] - 1):\n    for j in range(1, img.shape[1] - 1):\n        vizinhos = img[(i - 1) : (i + 2), (j - 1) : (j + 2)]\n        # multiplica pelo kernel e soma para gerar o novo valor\n        new_img[i, j] = np.sum(w * vizinhos)\n\nnew_img = new_img[1:-1, 1:-1]\n\nA imagem gerada depois da convolução foi:\n\nplt.imshow(new_img)\n\n<matplotlib.image.AxesImage at 0x7fb80deebd30>\n\n\n\n\n\nO Keras faz exatamente isso que o loop acima está fazendo quando usamos a camada Conv2D. Vamos reproduzir a mesma imagem, agora usando o Keras. Pontos importantes:\n\nO kernel para o Keras é um tensor com 4 dimensões: altura e largura do kernel, número de canais no input e número de filtros. Por isso tivemos que fazer o reshape.\nÉ recomendado usar ativações na convolução. Nesse exemplo não estamos usando para poder reproduzir o código acima.\nExistem diversas formas de padding. padding='valid' significa que não estamos fazendo padding, ou seja, só vamos passar pelos pixels que possuem todos os vizinhos.\nO parâmetro weights das camadas do Keras permite que você especifique os parâmetros iniciais manualmente.\nDesativamos o viés também para simplificar o loop acima.\n\n\nw2 = np.reshape(w, (3, 3, 1, 1)) # o Kernel para o keras é uma array com 4 dimensões\nconv = keras.layers.Conv2D(\n    filters=1,\n    kernel_size=(3, 3),\n    activation=\"linear\",\n    padding=\"valid\",\n    weights=[w2],\n    use_bias=False,\n)\n\nAgora aplicamos essa cada na imagem.\n\n# o Keras espera imagens no formato (nobs, altura, largura, canais)\nim = np.reshape(img, (1, 28, 28, 1)) \nout = conv(im)\n\nVocê pode comparar a imagem gerada pelo Keras e pelo nosso loop e verificar que são idênticas.\n\nplt.imshow(out.numpy()[0, :, :, 0])\n\n<matplotlib.image.AxesImage at 0x7fb80f1455b0>"
  },
  {
    "objectID": "07-conv-mnist.html",
    "href": "07-conv-mnist.html",
    "title": "MNIST",
    "section": "",
    "text": "from tensorflow import keras\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nO banco de dados do MNIST pode ser obtido usando funções prontas do Keras.\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nAgora podemos visualizar algumas imagens do banco de dados:\n\nplt.imshow(x_train[0])\nplt.show()\nplt.imshow(x_train[1])\nplt.show()\nplt.imshow(x_train[2])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOs vetores y_train e y_test possuem o valor representado pelo dígito que está na imagem.\n\nprint(y_train[0:3])\n\n[5 0 4]\n\n\nNote que as imagens do MNIST são tamanho 28x28x1 (o 1 vem do núemro de canais - como a imagem é P&B só temos 1 canal.)\n\nx_train.shape\n\n(60000, 28, 28)\n\n\nAgora vamos definir o modelo. Ponto importantes:\n\nO input do modelo tem formato 28x28x1. Isso é a altura e largura das imagens do input é 28 e a imagem é P&B, 1 canal.\nAs imagens estão representadas por número inteiros de 0 a 255, é importante fazer o rescaling e transformar esses valores em números entre 0 e 1, para não termos problemas com o algoritmo de otimização.\nEstamos usando 4 blocos de convolução/ pooling. Em cada um deles aumentamos o número de filtros da convolução e o ‘pooling’ será reponsável por diminuir o tamanho da imagem.\nO Flatten tira as dimensões que não estão sendo usadas e transforma o output em um vetor.\nNo final colocamos um MLP. Note que o número de outputs da última camada é 10, pois temos 10 classes possíveis (dígitos de 0 a 9). A ativação é softmax pois cada imagem pertence a uma única classe. Portanto as probabilidades por linha devem somar 1.\n\n\ninput = keras.layers.Input(shape=(28, 28, 1))\noutput = keras.layers.Rescaling(1.0/255.0)(input)\n\noutput = keras.layers.Conv2D(\n    kernel_size=(3, 3), filters=32, activation=\"relu\", padding=\"same\"\n)(output)\noutput = keras.layers.MaxPool2D(pool_size=(2, 2))(output)\n\noutput = keras.layers.Conv2D(\n    kernel_size=(3, 3), filters=64, activation=\"relu\", padding=\"same\"\n)(output)\noutput = keras.layers.MaxPool2D(pool_size=(2, 2))(output)\n\noutput = keras.layers.Conv2D(\n    kernel_size=(3, 3), filters=128, activation=\"relu\", padding=\"same\"\n)(output)\noutput = keras.layers.MaxPool2D(pool_size=(2, 2))(output)\n\noutput = keras.layers.Conv2D(\n    kernel_size=(3, 3), filters=256, activation=\"relu\", padding=\"same\"\n)(output)\noutput = keras.layers.MaxPool2D(pool_size=(2, 2))(output)\n\noutput = keras.layers.Flatten()(output)\noutput = keras.layers.Dense(128, activation=\"relu\")(output)\noutput = keras.layers.Dense(10, activation=\"softmax\")(output)\n\n2022-04-27 22:22:08.279452: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nAgora criamos o modelo do Keras\n\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n\n\n\nModel: \"model\"\n\n\n\n\n_________________________________________________________________\n\n\n\n\n Layer (type)                Output Shape              Param #   \n\n\n\n\n=================================================================\n\n\n\n\n input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n\n\n\n\n                                                                 \n\n\n\n\n rescaling (Rescaling)       (None, 28, 28, 1)         0         \n\n\n\n\n                                                                 \n\n\n\n\n conv2d (Conv2D)             (None, 28, 28, 32)        320       \n\n\n\n\n                                                                 \n\n\n\n\n max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n\n\n\n\n )                                                               \n\n\n\n\n                                                                 \n\n\n\n\n conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n\n\n\n\n                                                                 \n\n\n\n\n max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n\n\n\n\n 2D)                                                             \n\n\n\n\n                                                                 \n\n\n\n\n conv2d_2 (Conv2D)           (None, 7, 7, 128)         73856     \n\n\n\n\n                                                                 \n\n\n\n\n max_pooling2d_2 (MaxPooling  (None, 3, 3, 128)        0         \n\n\n\n\n 2D)                                                             \n\n\n\n\n                                                                 \n\n\n\n\n conv2d_3 (Conv2D)           (None, 3, 3, 256)         295168    \n\n\n\n\n                                                                 \n\n\n\n\n max_pooling2d_3 (MaxPooling  (None, 1, 1, 256)        0         \n\n\n\n\n 2D)                                                             \n\n\n\n\n                                                                 \n\n\n\n\n flatten (Flatten)           (None, 256)               0         \n\n\n\n\n                                                                 \n\n\n\n\n dense (Dense)               (None, 128)               32896     \n\n\n\n\n                                                                 \n\n\n\n\n dense_1 (Dense)             (None, 10)                1290      \n\n\n\n\n                                                                 \n\n\n\n\n=================================================================\n\n\n\n\nTotal params: 422,026\n\n\n\n\nTrainable params: 422,026\n\n\n\n\nNon-trainable params: 0\n\n\n\n\n_________________________________________________________________\n\n\n\n\nVeja o número de parâmetros da primeira camada de convolução. Você pode pensar da seguinte forma:\n\nCada kernel tem 3x3 parâmetros.\nCada filtro tem 1 kernel.\nCada filtro possui um viés.\nTotal: 3x3x32 + 32\n\nPara a segunda camada a conta é a mesma, mas o input agora não possui apenas um canal. Agora o input possui 32 canais então pensamos:\n\nCada kernel tem 3x3x32 parâmetros.\nCada filtro tem 1 kernel.\nCada filtro possui um viés.\nTotal: 3x3x32x64 + 64\n\nAgora podemos compilar o modelo. Usamos a ‘sparse_categorical_crossentropy’ pois o problema de classificação em muitas classes. O sparse é usado para não precisar transformar o vetor de resposta y_train em uma matriz com one-hot encode.\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n\nEntão, podemos ajustar o modelo:\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2,\n          verbose=2)\n\n\n\nEpoch 1/5\n\n\n\n\n1500/1500 - 62s - loss: 0.8062 - accuracy: 0.7558 - val_loss: 0.1782 - val_accuracy: 0.9439 - 62s/epoch - 41ms/step\n\n\n\n\nEpoch 2/5\n\n\n\n\n1500/1500 - 53s - loss: 0.1357 - accuracy: 0.9585 - val_loss: 0.0961 - val_accuracy: 0.9704 - 53s/epoch - 35ms/step\n\n\n\n\nEpoch 3/5\n\n\n\n\n1500/1500 - 56s - loss: 0.0929 - accuracy: 0.9711 - val_loss: 0.0938 - val_accuracy: 0.9715 - 56s/epoch - 37ms/step\n\n\n\n\nEpoch 4/5\n\n\n\n\n1500/1500 - 53s - loss: 0.0712 - accuracy: 0.9773 - val_loss: 0.0754 - val_accuracy: 0.9763 - 53s/epoch - 35ms/step\n\n\n\n\nEpoch 5/5\n\n\n\n\n1500/1500 - 51s - loss: 0.0596 - accuracy: 0.9812 - val_loss: 0.0613 - val_accuracy: 0.9815 - 51s/epoch - 34ms/step\n\n\n\n\n<keras.callbacks.History at 0x7fae30c74e80>\n\n\n\n\nVamos também verificar a matriz de confusão para a base de teste. Note que para encontrar a classe predita, pegamos a classe com maior probabilidade.\n\nmetrics.confusion_matrix(\n    y_test, np.argmax(model.predict(x_test), axis=1)\n)\n\narray([[ 965,    0,    1,    0,    2,    1,    1,    3,    3,    4],\n       [   0, 1126,    2,    0,    1,    1,    2,    2,    1,    0],\n       [   0,    0, 1024,    2,    0,    0,    0,    6,    0,    0],\n       [   0,    0,    0,  998,    0,    5,    0,    3,    3,    1],\n       [   0,    0,    1,    0,  979,    0,    1,    0,    0,    1],\n       [   1,    0,    0,    4,    1,  879,    2,    2,    0,    3],\n       [   2,    2,    1,    0,    7,    8,  937,    0,    1,    0],\n       [   0,    2,    6,    1,    1,    0,    0, 1013,    1,    4],\n       [   2,    0,    5,    8,    5,    5,    0,    6,  933,   10],\n       [   0,    3,    0,    2,   11,    1,    0,    4,    2,  986]])"
  },
  {
    "objectID": "08-tfhub.html",
    "href": "08-tfhub.html",
    "title": "Modelos pré-treinados",
    "section": "",
    "text": "from tensorflow import keras\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\n\nA base de dados que vamos usar será o CIFAR10 que pode ser carregada no Keras com:\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n\nPodemos visualizar algumas imagens: Agora podemos visualizar algumas imagens do banco de dados:\n\nplt.imshow(x_train[0])\nplt.show()\nplt.imshow(x_train[1])\nplt.show()\nplt.imshow(x_train[2])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVamos agora definir o modelo. Ponto importantes:\n\nCada modelo no tfhub possui uma documentação (exemplo) que vai falar o formato esperado da imagem. No caso deste MobileNet que estamos usando é esperado que a imagem tenha cores como valores entre 0 e 1 e que a imagem tenha tamanho 224x224.\nUsamos as camadas Rescaling e Resizing mudar as cores de inteiros de [1,255] em valores entre 0 e 1 e para aumentar o tamanho das imagens de 32x32 para 224x224.\nPara usar o modelo pré treinado, basta passar a sua URL para a camada hub.KerasLayer. Usamos trainable=False pois queremos deixar fixos os pesos deste modelo.\nO modelo pré-treinado está transformando cada imagem em um vetor de tamanho 1024. Em seguida usamos uma regressão multinomial p/ classificar nas 10 classes do CIFAR10.\n\n\ninput = keras.layers.Input(shape=(32, 32, 3))\nout = keras.layers.Rescaling(1 / 255.0)(input)\nout = keras.layers.Resizing(224, 224)(out)\nout = hub.KerasLayer(\n    handle=\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/5\",\n    trainable=False,\n)(out)\nout = keras.layers.Dropout(rate=0.2)(out)\nout = keras.layers.Dense(units=10, activation=\"softmax\")(out)\n\n2022-04-28 10:23:31.586158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nVeja que a camada do TFHUB possui todos os pesos do MobileNet.\n\nmodel = keras.Model(inputs=input, outputs=out)\nmodel.summary()\n\n\n\nModel: \"model\"\n\n\n\n\n_________________________________________________________________\n\n\n\n\n Layer (type)                Output Shape              Param #   \n\n\n\n\n=================================================================\n\n\n\n\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n\n\n\n\n                                                                 \n\n\n\n\n rescaling (Rescaling)       (None, 32, 32, 3)         0         \n\n\n\n\n                                                                 \n\n\n\n\n resizing (Resizing)         (None, 224, 224, 3)       0         \n\n\n\n\n                                                                 \n\n\n\n\n keras_layer (KerasLayer)    (None, 1024)              3228864   \n\n\n\n\n                                                                 \n\n\n\n\n dropout (Dropout)           (None, 1024)              0         \n\n\n\n\n                                                                 \n\n\n\n\n dense (Dense)               (None, 10)                10250     \n\n\n\n\n                                                                 \n\n\n\n\n=================================================================\n\n\n\n\nTotal params: 3,239,114\n\n\n\n\nTrainable params: 10,250\n\n\n\n\nNon-trainable params: 3,228,864\n\n\n\n\n_________________________________________________________________\n\n\n\n\nAgora podemos compilar o modelo:\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    metrics=[\"accuracy\"],\n)\n\nEstamos usando a mesma loss que usamos no exemplo anterior. Agora vamos ajustar o modelo. É esperado termos um acerto bem maior do que se tivéssemos ajustado um modelo do zero, pois o modelo pré treinado foi ajustado em banco de dados muito maiores e por isso tem bastante informação p/ agregar.\n\nhist = model.fit(x_train, y_train, epochs=5, validation_split=0.2, verbose=2)\n\n\n\nEpoch 1/5\n\n\n\n\n1250/1250 - 1259s - loss: 0.6180 - accuracy: 0.7867 - val_loss: 0.3967 - val_accuracy: 0.8616 - 1259s/epoch - 1s/step\n\n\n\n\nEpoch 2/5\n\n\n\n\n1250/1250 - 1133s - loss: 0.4270 - accuracy: 0.8516 - val_loss: 0.3835 - val_accuracy: 0.8634 - 1133s/epoch - 907ms/step\n\n\n\n\nEpoch 3/5\n\n\n\n\n1250/1250 - 1250s - loss: 0.4012 - accuracy: 0.8590 - val_loss: 0.3635 - val_accuracy: 0.8702 - 1250s/epoch - 1000ms/step\n\n\n\n\nEpoch 4/5\n\n\n\n\n1250/1250 - 1218s - loss: 0.3934 - accuracy: 0.8648 - val_loss: 0.3588 - val_accuracy: 0.8717 - 1218s/epoch - 974ms/step\n\n\n\n\nEpoch 5/5\n\n\n\n\n1250/1250 - 1204s - loss: 0.3822 - accuracy: 0.8676 - val_loss: 0.3640 - val_accuracy: 0.8719 - 1204s/epoch - 963ms/step"
  },
  {
    "objectID": "09-text-vectorization.html",
    "href": "09-text-vectorization.html",
    "title": "Pré-processamento",
    "section": "",
    "text": "Para isso, usamos a camada TextVectorization. Note que dsta camada é um pouco diferente das demais, pois, precisamos ‘adaptá-la’ antes de usar.\nO parâmetro max_tokens diz o número máximo de palavras que a camada vai guardar no seu vocabulário. Isso é usado quando não queremos que palavras que aparecem muito pouco tenham um número inteiro atribuido.\nO output_mode='int' indica que queremos transformar cada palavra em um número inteiro. Existem outras formas de vetorizar (por exemplo tf-idf) mas não vamos usá-las em deep learning.\n\nfrom tensorflow import keras\nlayer = keras.layers.TextVectorization(max_tokens=10, output_mode=\"int\",)\n\n2022-05-01 10:30:26.275255: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nAgora vamos adaptar a camada para um conjunto de frases:\n\nfrases = [\n    \"eu gosto de gatos\",\n    \"eu gosto de cachorros\",\n    \"eu gosto de gatos e cachorros\",\n]\n\nlayer.adapt(frases)\n\nEntão podemos usá-la:\n\nlayer(frases)\n\n<tf.Tensor: shape=(3, 6), dtype=int64, numpy=\narray([[3, 2, 4, 5, 0, 0],\n       [3, 2, 4, 6, 0, 0],\n       [3, 2, 4, 5, 7, 6]])>\n\n\nE obter o vocabulário:\n\nvocab = layer.get_vocabulary()\nprint(vocab)\n\n['', '[UNK]', 'gosto', 'eu', 'de', 'gatos', 'cachorros', 'e']\n\n\nNote que no vocabulário, sempre temos que o primeiro ítem é '' e o segundo é <UNK>. Esses tokens servem para padding e para palavras desconhecidas pela camada de vetorização respectivamente.\nPor exemplo, se fizermos:\n\nlayer(\"eu gosto de tubarão\")\n\n<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 2, 4, 1])>\n\n\nVeja que a palavra ‘tubarao’ foi substituida pelo inteiro representado o <UNK> pois essa palavra não aparecia quando adaptamos a camada."
  },
  {
    "objectID": "10-embedding.html",
    "href": "10-embedding.html",
    "title": "Word embeddings",
    "section": "",
    "text": "Para isso, usamos a camada TextVectorization. Note que dsta camada é um pouco diferente das demais, pois, precisamos ‘adaptá-la’ antes de usar.\nO parâmetro max_tokens diz o número máximo de palavras que a camada vai guardar no seu vocabulário. Isso é usado quando não queremos que palavras que aparecem muito pouco tenham um número inteiro atribuido.\nO output_mode='int' indica que queremos transformar cada palavra em um número inteiro. Existem outras formas de vetorizar (por exemplo tf-idf) mas não vamos usá-las em deep learning.\n\nfrom tensorflow import keras\nlayer = keras.layers.TextVectorization(max_tokens=10, output_mode=\"int\",)\n\nAgora vamos adaptar a camada para um conjunto de frases:\n\nfrases = [\n    \"eu gosto de gatos\",\n    \"eu gosto de cachorros\",\n    \"eu gosto de gatos e cachorros\",\n]\n\nlayer.adapt(frases)\nvocab = layer.get_vocabulary()\n\nWARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7feca7c26670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\nAgora podemos definir um input, passar a camada de vetorização e em seguida definir a camada de embeddings. O parâmetro input_dim da camada Embedding indica o número de tokens possíveis. No nosso caso, esse número é o tamanho do vocabulário. A camada de embedding faz cada palavra ser representada por um vetorizinho. O parâmetro output_dim indica o tamanho desse vetor. No exemplo, cada palavra será representada por um vetor de tamanho 2.\n\ninput = keras.layers.Input(shape=(), dtype=\"string\")\nvec = layer(input)\noutput = keras.layers.Embedding(input_dim=len(vocab), output_dim=2)(vec)\n\nAgora podemos criar um modelo:\n\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n\nModel: \"model_3\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n input_5 (InputLayer)        [(None,)]                 0         \n\n\n                                                                 \n\n\n text_vectorization_5 (TextV  (None, None)             0         \n\n\n ectorization)                                                   \n\n\n                                                                 \n\n\n embedding_4 (Embedding)     (None, None, 2)           16        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 16\n\n\nTrainable params: 16\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\nA camada de embeddings possui len(vocab)*2 parâmetros. Isto é, 2 parâmetros para cada palavra do vocabulário.\nAgora vamos ver como uma frase fica representada pela camada de embeddings:\n\nmodel.predict(frases)[0]\n\narray([[ 0.04450825,  0.04365969],\n       [ 0.00506002, -0.00034119],\n       [-0.00431657, -0.03885051],\n       [ 0.04931844,  0.03272061],\n       [ 0.00938363, -0.02423037],\n       [ 0.00938363, -0.02423037]], dtype=float32)\n\n\nTambém podemos observar a matriz de embeddings:\n\nmodel.get_weights()[1]\n\narray([[ 0.00938363, -0.02423037],\n       [-0.04308408,  0.00859753],\n       [ 0.00506002, -0.00034119],\n       [ 0.04450825,  0.04365969],\n       [-0.00431657, -0.03885051],\n       [ 0.04931844,  0.03272061],\n       [ 0.03986887,  0.01469808],\n       [-0.04277081,  0.01205132]], dtype=float32)\n\n\nCada linha aqui é um vetor que representa uma palavra do vocabulário. A order é a mesma que aparece em vocab:\n\nprint(vocab)\n\n['', '[UNK]', 'gosto', 'eu', 'de', 'gatos', 'cachorros', 'e']"
  },
  {
    "objectID": "11-avg-pooling.html",
    "href": "11-avg-pooling.html",
    "title": "Pooling",
    "section": "",
    "text": "Em seguida vamos considerar que cada texto (no caso do banco de dados, comentários feiros em forum da internet) pode ser representado pela média das palavras que têm nele. Usar a média é também chamado de GlobalAveragePooling no Keras:\n\nfrom tensorflow import keras\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nVamos carregar o banco de dados:\n\ndataset = pd.read_csv(\n    \"https://storage.googleapis.com/deep-learning-com-r/toxic-comments.csv\"\n)\ndataset.head()\n\n\n\n\n  \n    \n      \n      id\n      comment_text\n      toxic\n      severe_toxic\n      obscene\n      threat\n      insult\n      identity_hate\n    \n  \n  \n    \n      0\n      0000997932d777bf\n      Explanation\\nWhy the edits made under my usern...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      000103f0d9cfb60f\n      D'aww! He matches this background colour I'm s...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      000113f07ec002fd\n      Hey man, I'm really not trying to edit war. It...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0001b41b1c6bb37e\n      \"\\nMore\\nI can't make any real suggestions on ...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0001d958c54c6e35\n      You, sir, are my hero. Any chance you remember...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\nNosso objetivo será, a partir do texto do comentário, classificar em tóxico ou não tóxico.\n\nx = dataset['comment_text'].to_numpy()\ny = dataset.iloc[:,2].to_numpy()\n\nPara escolher o valor de output_sequence_length, isto é, o número máximo de palavras em cada texto, fazemos um histograma do número de palavras por texto:\n\nnumero_palavras = [len(i.split()) for i in x]\nsns.histplot(numero_palavras)\n\n<AxesSubplot:ylabel='Count'>\n\n\n\n\n\nComo a maioria dos textos possui menos do que 150 palavras, escolhemos que o tamanho máximo é 150. Dessa forma, se um texto for muito grande, usamos apenas as primeiras 150 palavras para classificar em tóxico/não tóxico. Isso vai permitir que o modelo fique muito mais rápido.\nVamos então criar a camada de vetorização e adaptá-la:\n\nvectorize = keras.layers.TextVectorization(\n    max_tokens=10000, output_mode=\"int\", output_sequence_length=150\n)\n\nvectorize.adapt(x)\nvocab = vectorize.get_vocabulary()\n\nAgora vamos definir o modelo. Lembre que cada palavra será representada por um vetor pela camada de embedding e queremos que a média desses valores represente a probabilidade do texto ser tóxicou ou não.\n\ninput = keras.layers.Input(shape=(), dtype=\"string\")\noutput = vectorize(input)\noutput = keras.layers.Embedding(input_dim=len(vocab), output_dim=1)(output)\noutput = keras.layers.GlobalAveragePooling1D()(output)\noutput = keras.layers.Activation(\"sigmoid\")(output)\n\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n\nModel: \"model_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n input_2 (InputLayer)        [(None,)]                 0         \n\n\n                                                                 \n\n\n text_vectorization_1 (TextV  (None, 150)              0         \n\n\n ectorization)                                                   \n\n\n                                                                 \n\n\n embedding_1 (Embedding)     (None, 150, 1)            10000     \n\n\n                                                                 \n\n\n global_average_pooling1d_1   (None, 1)                0         \n\n\n (GlobalAveragePooling1D)                                        \n\n\n                                                                 \n\n\n activation_1 (Activation)   (None, 1)                 0         \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 10,000\n\n\nTrainable params: 10,000\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\nNesse modelo, o número de parâmetros é igual ao número de palavras que temos no vocabulário.\nAgora podemos compilar o modelo. Vamos usar a ‘binary_crossentropy’ pois o problema é de classificação binária. Além da acurácia, vamos medir o AUC, uma vez que o problema é bem desbalanceado (muitos zeros e poucos uns).\n\nauc = keras.metrics.AUC(curve=\"ROC\")\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", auc])\n\nAgora vamos ajustar o modelo:\n\nmodel.fit(x, y, epochs=5, batch_size=32, validation_split=0.2, verbose=2)\n\n\n\nEpoch 1/5\n\n\n\n\n3990/3990 - 6s - loss: 0.4087 - accuracy: 0.9041 - auc_1: 0.5017 - val_loss: 0.3155 - val_accuracy: 0.9051 - val_auc_1: 0.5966 - 6s/epoch - 1ms/step\n\n\n\n\nEpoch 2/5\n\n\n\n\n3990/3990 - 5s - loss: 0.3060 - accuracy: 0.9044 - auc_1: 0.7704 - val_loss: 0.2966 - val_accuracy: 0.9053 - val_auc_1: 0.8276 - 5s/epoch - 1ms/step\n\n\n\n\nEpoch 3/5\n\n\n\n\n3990/3990 - 6s - loss: 0.2939 - accuracy: 0.9045 - auc_1: 0.8131 - val_loss: 0.2884 - val_accuracy: 0.9055 - val_auc_1: 0.8167 - 6s/epoch - 1ms/step\n\n\n\n\nEpoch 4/5\n\n\n\n\n3990/3990 - 6s - loss: 0.2871 - accuracy: 0.9047 - auc_1: 0.8222 - val_loss: 0.2826 - val_accuracy: 0.9057 - val_auc_1: 0.8313 - 6s/epoch - 2ms/step\n\n\n\n\nEpoch 5/5\n\n\n\n\n3990/3990 - 7s - loss: 0.2816 - accuracy: 0.9049 - auc_1: 0.8385 - val_loss: 0.2776 - val_accuracy: 0.9058 - val_auc_1: 0.8462 - 7s/epoch - 2ms/step\n\n\n\n\n<keras.callbacks.History at 0x7f9c47499e50>\n\n\n\n\nPodemos indetificar as palavras com os maiores e menores valores de embedding. Como o texto é classificado em tóxico/não tóxico com base na média dos valores das palavras dele, palavras com valores altos ajudam a classificar o texto como tóxico e palavras com valores baixos ajudam a considerar o texto como não tóxico.\n\nembeddings = model.get_weights()[1]\nwords = pd.DataFrame.from_dict({\n  \"word\": vocab,\n  \"embedding\": embeddings[:,0]\n})\n\nAs 10 palavras com maires valores de embeddings são:\n\nwords.sort_values(\"embedding\", ascending = False).head(10)\n\n\n\n\n  \n    \n      \n      word\n      embedding\n    \n  \n  \n    \n      382\n      fucking\n      8.187375\n    \n    \n      139\n      fuck\n      7.190009\n    \n    \n      563\n      stupid\n      6.245700\n    \n    \n      318\n      shit\n      5.337469\n    \n    \n      1190\n      idiot\n      5.320159\n    \n    \n      762\n      bitch\n      5.001073\n    \n    \n      741\n      hell\n      4.981386\n    \n    \n      224\n      off\n      4.903869\n    \n    \n      7\n      you\n      4.426858\n    \n    \n      306\n      life\n      4.257237\n    \n  \n\n\n\n\nE as 10 palavras com menores valores são:\n\nwords.sort_values(\"embedding\", ascending = True).head(10)\n\n\n\n\n  \n    \n      \n      word\n      embedding\n    \n  \n  \n    \n      24\n      article\n      -9.915396\n    \n    \n      124\n      thank\n      -9.775599\n    \n    \n      46\n      please\n      -9.449037\n    \n    \n      2\n      the\n      -9.399350\n    \n    \n      86\n      may\n      -8.358996\n    \n    \n      13\n      for\n      -8.049402\n    \n    \n      18\n      as\n      -7.831261\n    \n    \n      191\n      welcome\n      -7.755289\n    \n    \n      94\n      thanks\n      -7.719523\n    \n    \n      17\n      be\n      -7.713208\n    \n  \n\n\n\n\nVemos que os embeddings que são comportaram como imaginávamos. Maiores valores indicam palavras que levam à textos tóxicos e valores menores levam a textos não tóxicos."
  },
  {
    "objectID": "12-simple-rnn.html",
    "href": "12-simple-rnn.html",
    "title": "RNN’s",
    "section": "",
    "text": "import numpy as np\nfrom tensorflow import keras\n\nVamos gerar sequências de números aleatórios que podem ser crescentes ou decrescentes:\n\nDefinimos se a sequência vai ser crescente ou decrescente,\nPara cada sequencia, geramos l números aleatórios.\nSe a sequência for crescente, ordenamos os números de forma crescente, se não for, ordenamos de forma decrescente.\n\n\nn = 10000\nl = 10\n\ncresc = np.random.randint(0, 2, size=(n,))\nx = np.empty((n, l), dtype=np.float32)\nfor i, cr in enumerate(cresc):\n    tmp = np.random.uniform(size=(l,))\n    if cr == 1:\n        x[i, :] = tmp[np.argsort(tmp)]\n    else:\n        x[i, :] = tmp[np.argsort(-tmp)]\nx = x.reshape((n, l, 1))\n\nAgora vamos definir o modelo no Keras. Não vamos nos preocupar com os parâmetros nem nada ainda. A seguir vamos mostrar exatamente as contas que o Keras está fazendo por trás.\n\ninput = keras.layers.Input(shape=(l, 1))\noutput = keras.layers.SimpleRNN(units=1, activation=\"tanh\", use_bias=False)(input)\noutput = keras.layers.Activation(\"sigmoid\")(output)\nmodel = keras.Model(inputs=input, outputs=output)\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=\"accuracy\")\n\n2022-05-01 14:49:29.471829: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nAgora ajustamos o modelo:\n\nmodel.fit(x=x, y=cresc, epochs=10, verbose=2)\n\n\n\nEpoch 1/10\n\n\n\n\n313/313 - 1s - loss: 0.5114 - accuracy: 0.8961 - 856ms/epoch - 3ms/step\n\n\n\n\nEpoch 2/10\n\n\n\n\n313/313 - 0s - loss: 0.4128 - accuracy: 0.9780 - 316ms/epoch - 1ms/step\n\n\n\n\nEpoch 3/10\n\n\n\n\n313/313 - 0s - loss: 0.4038 - accuracy: 0.9721 - 308ms/epoch - 984us/step\n\n\n\n\nEpoch 4/10\n\n\n\n\n313/313 - 0s - loss: 0.4014 - accuracy: 0.9684 - 314ms/epoch - 1ms/step\n\n\n\n\nEpoch 5/10\n\n\n\n\n313/313 - 0s - loss: 0.4009 - accuracy: 0.9679 - 309ms/epoch - 986us/step\n\n\n\n\nEpoch 6/10\n\n\n\n\n313/313 - 0s - loss: 0.3986 - accuracy: 0.9676 - 311ms/epoch - 992us/step\n\n\n\n\nEpoch 7/10\n\n\n\n\n313/313 - 0s - loss: 0.3972 - accuracy: 0.9698 - 309ms/epoch - 988us/step\n\n\n\n\nEpoch 8/10\n\n\n\n\n313/313 - 0s - loss: 0.3977 - accuracy: 0.9678 - 314ms/epoch - 1ms/step\n\n\n\n\nEpoch 9/10\n\n\n\n\n313/313 - 0s - loss: 0.3966 - accuracy: 0.9664 - 313ms/epoch - 1000us/step\n\n\n\n\nEpoch 10/10\n\n\n\n\n313/313 - 0s - loss: 0.3984 - accuracy: 0.9668 - 316ms/epoch - 1ms/step\n\n\n\n\n<keras.callbacks.History at 0x7fbe99f1a6a0>\n\n\n\n\nVimos que a RNN consegue prever bem se a sequência é crescente ou decrescente.\nAgora vamos ver exatamente as contas que o Keras faz. Primeiro, definimos a função sigmoid.\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nDepois, pegamos o valor do peso estimado pelo Keras:\n\nw_rnn = model.layers[1].get_weights()\n\nVamos escolher a primeira observação, para mostrar a conta:\n\nx_ = x[0]\n\nAgora o loop que acontece dentro da RNN. Começando com um estado inicial s=0, fazemos:\n\ns = 0.0\nfor i in range(l):\n    s = np.tanh(w_rnn[0] * x_[i] + w_rnn[1] * s)\nsigmoid(s)\n\narray([[0.72884184]], dtype=float32)\n\n\nEm cada passo o estado é atualizado com um peso para o estado anterior e outro para a nova observação, passando por uma função de ativação. Podemos comparar isso com o resultado do Keras:\n\nmodel.predict(x)[0]\n\narray([0.7288419], dtype=float32)"
  },
  {
    "objectID": "13-simple-lstm.html",
    "href": "13-simple-lstm.html",
    "title": "LSTM’s",
    "section": "",
    "text": "import numpy as np\nfrom tensorflow import keras\n\nPrimeiro, vamos gerar os dados novamente:\n\nn = 10000\nl = 10\n\ncresc = np.random.randint(0, 2, size=(n,))\nx = np.empty((n, l), dtype=np.float32)\nfor i, cr in enumerate(cresc):\n    tmp = np.random.uniform(size=(l,))\n    if cr == 1:\n        x[i, :] = tmp[np.argsort(tmp)]\n    else:\n        x[i, :] = tmp[np.argsort(-tmp)]\nx = x.reshape((n, l, 1))\n\nAgora vamos definir o modelo no Keras. Não vamos nos preocupar com os parâmetros nem nada ainda. A seguir vamos mostrar exatamente as contas que o Keras está fazendo por trás.\n\ninput = keras.layers.Input(shape=(l, 1))\noutput = keras.layers.LSTM(units=1, activation=\"tanh\", use_bias=False, recurrent_activation=\"sigmoid\")(input)\noutput = keras.layers.Activation(\"sigmoid\")(output)\nmodel = keras.Model(inputs=input, outputs=output)\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=\"accuracy\")\n\n2022-05-01 15:00:16.590139: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nAgora ajustamos o modelo:\n\nmodel.fit(x=x, y=cresc, epochs=10, batch_size=32, verbose=2)\n\n\n\nEpoch 1/10\n\n\n\n\n313/313 - 1s - loss: 0.5692 - accuracy: 0.5122 - 1s/epoch - 4ms/step\n\n\n\n\nEpoch 2/10\n\n\n\n\n313/313 - 0s - loss: 0.5240 - accuracy: 0.5884 - 482ms/epoch - 2ms/step\n\n\n\n\nEpoch 3/10\n\n\n\n\n313/313 - 0s - loss: 0.5175 - accuracy: 0.6381 - 488ms/epoch - 2ms/step\n\n\n\n\nEpoch 4/10\n\n\n\n\n313/313 - 0s - loss: 0.5116 - accuracy: 0.6825 - 489ms/epoch - 2ms/step\n\n\n\n\nEpoch 5/10\n\n\n\n\n313/313 - 0s - loss: 0.5062 - accuracy: 0.7285 - 488ms/epoch - 2ms/step\n\n\n\n\nEpoch 6/10\n\n\n\n\n313/313 - 0s - loss: 0.5030 - accuracy: 0.7500 - 483ms/epoch - 2ms/step\n\n\n\n\nEpoch 7/10\n\n\n\n\n313/313 - 0s - loss: 0.5005 - accuracy: 0.7698 - 488ms/epoch - 2ms/step\n\n\n\n\nEpoch 8/10\n\n\n\n\n313/313 - 0s - loss: 0.5011 - accuracy: 0.7617 - 484ms/epoch - 2ms/step\n\n\n\n\nEpoch 9/10\n\n\n\n\n313/313 - 0s - loss: 0.5017 - accuracy: 0.7678 - 488ms/epoch - 2ms/step\n\n\n\n\nEpoch 10/10\n\n\n\n\n313/313 - 0s - loss: 0.5001 - accuracy: 0.7730 - 487ms/epoch - 2ms/step\n\n\n\n\n<keras.callbacks.History at 0x7fee81e2ac70>\n\n\n\n\nAgora vamos ver a conta linha por linha que é feita pelo Keras para cada observação. Definimos as funções que vamos usar:\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nSelecionamos uma observação:\n\nx_ = x[0]\n\nObtemos os pesos estimados pelo Keras:\n\nw = model.get_weights()\n\nInicializamos os estados s e c e dentro do loop escrevemos a regra de atualização dos estados:\n\ns = 0\nc = 0\n\n\nfor t in range(l):\n    f = sigmoid(w[1][0, 1] * s + w[0][0, 1] * x_[t])\n    i = sigmoid(w[1][0, 0] * s + w[0][0, 0] * x_[t])\n    c_hat = np.tanh(w[1][0, 2] * s + w[0][0, 2] * x_[t])\n    c = f * c + i * c_hat\n    o = sigmoid(s * w[1][0, 3] + w[0][0, 3] * x_[t])\n    s = o * np.tanh(c)\n\nEmfim, aplicamos o sigmoid no estado final:\n\nsigmoid(s)\n\narray([0.73093957], dtype=float32)\n\n\nO valor predito pelo keras para essa observação seria:\n\nmodel.predict(x)[0]\n\narray([0.7309395], dtype=float32)"
  },
  {
    "objectID": "14-lstm.html",
    "href": "14-lstm.html",
    "title": "Aplicando a LSTM",
    "section": "",
    "text": "from tensorflow import keras\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nVamos carregar o banco de dados:\n\ndataset = pd.read_csv(\n    \"https://storage.googleapis.com/deep-learning-com-r/toxic-comments.csv\"\n)\ndataset.head()\n\n\n\n\n  \n    \n      \n      id\n      comment_text\n      toxic\n      severe_toxic\n      obscene\n      threat\n      insult\n      identity_hate\n    \n  \n  \n    \n      0\n      0000997932d777bf\n      Explanation\\nWhy the edits made under my usern...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      000103f0d9cfb60f\n      D'aww! He matches this background colour I'm s...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      000113f07ec002fd\n      Hey man, I'm really not trying to edit war. It...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0001b41b1c6bb37e\n      \"\\nMore\\nI can't make any real suggestions on ...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0001d958c54c6e35\n      You, sir, are my hero. Any chance you remember...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\nNosso objetivo será, a partir do texto do comentário, classificar nas diversas tags possíveis.\n\nx = dataset['comment_text'].to_numpy()\ny = dataset.iloc[:,2:].to_numpy()\nprint(dataset.columns[2:])\n\nIndex(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n       'identity_hate'],\n      dtype='object')\n\n\nVamos então criar a camada de vetorização e adaptá-la:\n\nvectorize = keras.layers.TextVectorization(\n    max_tokens=10000, output_mode=\"int\", output_sequence_length=150\n)\n\nvectorize.adapt(x)\nvocab = vectorize.get_vocabulary()\n\nEm seguida podemos criar o modelo no Keras:\n\ninput = keras.layers.Input(shape=(), dtype=\"string\")\noutput = vectorize(input)\noutput = keras.layers.Embedding(input_dim=len(vocab), output_dim=2)(output)\noutput = keras.layers.LSTM(units=256)(output)\noutput = keras.layers.Dense(units=y.shape[1], activation=\"sigmoid\")(output)\n\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n\nModel: \"model_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n input_3 (InputLayer)        [(None,)]                 0         \n\n\n                                                                 \n\n\n text_vectorization_2 (TextV  (None, 150)              0         \n\n\n ectorization)                                                   \n\n\n                                                                 \n\n\n embedding_2 (Embedding)     (None, 150, 2)            20000     \n\n\n                                                                 \n\n\n lstm_2 (LSTM)               (None, 256)               265216    \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 6)                 1542      \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 286,758\n\n\nTrainable params: 286,758\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\nNote que na última camada densa temos units=y.shape[1], isto é, um output para cada tag possível. A ativação é sigmoid, pois queremos prever uma probabilidade para cada coluna.\nAgora vamos compilar e ajustar o modelo;\n\nauc = keras.metrics.AUC(curve=\"ROC\")\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\", auc])\n\nmodel.fit(x, y, epochs=5, batch_size=32, validation_split=0.2, verbose=2)\n\n\n\nEpoch 1/5\n\n\n\n\n3990/3990 - 1162s - loss: 0.1694 - accuracy: 0.9937 - auc_1: 0.7032 - val_loss: 0.1410 - val_accuracy: 0.9941 - val_auc_1: 0.7484 - 1162s/epoch - 291ms/step\n\n\n\n\nEpoch 2/5\n\n\n\n\n3990/3990 - 12937s - loss: 0.1412 - accuracy: 0.9942 - auc_1: 0.7498 - val_loss: 0.1409 - val_accuracy: 0.9941 - val_auc_1: 0.7484 - 12937s/epoch - 3s/step\n\n\n\n\nEpoch 3/5\n\n\n\n\n3990/3990 - 635s - loss: 0.1412 - accuracy: 0.9942 - auc_1: 0.7494 - val_loss: 0.1410 - val_accuracy: 0.9941 - val_auc_1: 0.7483 - 635s/epoch - 159ms/step\n\n\n\n\nEpoch 4/5\n\n\n\n\n3990/3990 - 686s - loss: 0.1411 - accuracy: 0.9942 - auc_1: 0.7501 - val_loss: 0.1409 - val_accuracy: 0.9941 - val_auc_1: 0.7472 - 686s/epoch - 172ms/step\n\n\n\n\nEpoch 5/5\n\n\n\n\n3990/3990 - 712s - loss: 0.1411 - accuracy: 0.9942 - auc_1: 0.7495 - val_loss: 0.1409 - val_accuracy: 0.9941 - val_auc_1: 0.7481 - 712s/epoch - 178ms/step\n\n\n\n\n<keras.callbacks.History at 0x7fe8a3f70bb0>"
  },
  {
    "objectID": "15-nlp-tfhub.html",
    "href": "15-nlp-tfhub.html",
    "title": "Modelo pré-treinado",
    "section": "",
    "text": "from tensorflow import keras\nimport tensorflow_text\nimport tensorflow_hub as hub\nimport pandas as pd\nimport numpy as np\n\nA tarefa será bem similar com o que fizemos quando usamos modelos pré treinados para imagens.\nEm primeiro lugar vamos obter o banco de dados:\n\ndataset = pd.read_csv(\n    \"https://storage.googleapis.com/deep-learning-com-r/toxic-comments.csv\"\n)\ndataset.head()\n\n\n\n\n  \n    \n      \n      id\n      comment_text\n      toxic\n      severe_toxic\n      obscene\n      threat\n      insult\n      identity_hate\n    \n  \n  \n    \n      0\n      0000997932d777bf\n      Explanation\\nWhy the edits made under my usern...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      000103f0d9cfb60f\n      D'aww! He matches this background colour I'm s...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      000113f07ec002fd\n      Hey man, I'm really not trying to edit war. It...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0001b41b1c6bb37e\n      \"\\nMore\\nI can't make any real suggestions on ...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      0001d958c54c6e35\n      You, sir, are my hero. Any chance you remember...\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\nOs modelos pré-treinados do tfhub já incluem as suas camadas de pré-processamento, portanto não precisamos nos preocupar em pré-processar os textos usando a camada de vetorização. Eles também incluem os embeddings, portanto não vamos criar essas camadas. Muitos dos modelos também retornam embeddings completas para os textos, não apenas para as palavras.\n\nx = dataset[\"comment_text\"].to_numpy()\ny = dataset.iloc[:, 2:].to_numpy()\n\nVamos definir o modelo no Keras:\n\ninput = keras.layers.Input(shape=(), dtype=\"string\")\nencoded = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")(\n    input\n)\nencoded = hub.KerasLayer(\n    \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\", trainable=False\n)(encoded)\n\noutput = keras.layers.Dense(units=y.shape[1], activation=\"sigmoid\")(encoded[\"default\"])\n\nO modelo pode ser definido com:\n\nmodel = keras.Model(inputs=input, outputs=output)\n\nAgora podemos ajustar o modelo. Basicamente só estamos ajustando classificador que se baseia nas embeddings criadas pelo BERT.\n\nauc = keras.metrics.AUC(curve=\"ROC\")\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\", auc])\n\nmodel.fit(x, y, epochs=1, batch_size=32, validation_split=0.2, verbose=1)\n\n\n\n\n3990/3990 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9733 - auc: 0.8957\nO modelo demora para rodar. Mas em apenas uma época conseguimos um bom resultado."
  },
  {
    "objectID": "16-quora.html",
    "href": "16-quora.html",
    "title": "Quora",
    "section": "",
    "text": "import pandas as pd\nfrom tensorflow import keras\nimport numpy as np\n\nPrimeiro baixamos o banco de dados:\n\ndataset = pd.read_csv(\"https://storage.googleapis.com/deep-learning-com-r/quora.csv.zip\")\n\nVeja alguns dos pares de perguntas:\n\ndataset.head(5)\n\n\n\n\n  \n    \n      \n      id\n      qid1\n      qid2\n      question1\n      question2\n      is_duplicate\n    \n  \n  \n    \n      0\n      0\n      1\n      2\n      What is the step by step guide to invest in sh...\n      What is the step by step guide to invest in sh...\n      0\n    \n    \n      1\n      1\n      3\n      4\n      What is the story of Kohinoor (Koh-i-Noor) Dia...\n      What would happen if the Indian government sto...\n      0\n    \n    \n      2\n      2\n      5\n      6\n      How can I increase the speed of my internet co...\n      How can Internet speed be increased by hacking...\n      0\n    \n    \n      3\n      3\n      7\n      8\n      Why am I mentally very lonely? How can I solve...\n      Find the remainder when [math]23^{24}[/math] i...\n      0\n    \n    \n      4\n      4\n      9\n      10\n      Which one dissolve in water quikly sugar, salt...\n      Which fish would survive in salt water?\n      0\n    \n  \n\n\n\n\nVamos arrumar um pouquinho os dados, colocando em variáveis:\n\np1 = dataset[\"question1\"].to_numpy().astype(\"str\")\np2 = dataset[\"question2\"].to_numpy().astype(\"str\")\ny = dataset[\"is_duplicate\"].to_numpy().astype(\"float\")\n\nEntão podemos definir a camada de vetorização. Lembre-se que essa camada precisa ser ‘adaptada’ antes de ser usada.\n\nvectorize = keras.layers.TextVectorization(max_tokens=50000, output_mode=\"int\",\n            pad_to_max_tokens=True)\nvectorize.adapt(np.append(p1, p2))\n\n2022-05-02 09:29:31.386461: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nAgora vamos definir o modelo:\n\nEsse modelo possui dois inputs: a pergunta 1 e a pergunta 2.\nQueremos que o nosso modelo retorne a probabilidade das duas perguntas serem duplicadas.\nQueremos que para o modelo não importe a ordem das perguntas. Isto é, se trocarmos a pergunta 1 pela pergunta 2 deveria dar a mesma probabilidade.\n\nVamos definir o modelo no Keras:\n\npergunta1 = keras.layers.Input(shape=(), dtype=\"string\")\npergunta2 = keras.layers.Input(shape=(), dtype=\"string\")\n\nAgora, queremos colocar camadas capazes de transformar cada pergunta em um vetor. Poderíamos usar um modelo pré-treinado, mas nesse exemplo vamos fazer do zero.\n\ninput = keras.layers.Input(shape=(), dtype=\"string\")\nencoded = vectorize(input)\nencoded = keras.layers.Embedding(len(vectorize.get_vocabulary()), 256)(encoded)\nencoded = keras.layers.LSTM(units=128)(encoded)\nencode_sequence = keras.Model(inputs=input, outputs=encoded)\n\nAgora que definimos o submodelo, vamos continuar a definição do modelo inicial.\n\nvetor_pergunta1 = encode_sequence(pergunta1)\nvetor_pergunta2 = encode_sequence(pergunta2)\n\nvetor_pergunta(1|2) é um vetor que representa cada uma das perguntas. Podemos calcular uma medida de distância entre esses dois vetores para determinar se as perguntas são parecidas ou não.\n\noutput = keras.layers.Dot(axes=1)([vetor_pergunta1, vetor_pergunta2])\noutput = keras.layers.Dense(units=1, activation=\"sigmoid\")(output)\n\nPodemos então definir o modelo:\n\nmodel = keras.Model(inputs=[pergunta1, pergunta2], outputs=output)\nmodel.summary()\n\nModel: \"model_1\"\n\n\n__________________________________________________________________________________________________\n\n\n Layer (type)                   Output Shape         Param #     Connected to                     \n\n\n==================================================================================================\n\n\n input_7 (InputLayer)           [(None,)]            0           []                               \n\n\n                                                                                                  \n\n\n input_8 (InputLayer)           [(None,)]            0           []                               \n\n\n                                                                                                  \n\n\n model (Functional)             (None, 128)          12997120    ['input_7[0][0]',                \n\n\n                                                                  'input_8[0][0]']                \n\n\n                                                                                                  \n\n\n dot (Dot)                      (None, 1)            0           ['model[0][0]',                  \n\n\n                                                                  'model[1][0]']                  \n\n\n                                                                                                  \n\n\n dense (Dense)                  (None, 1)            2           ['dot[0][0]']                    \n\n\n                                                                                                  \n\n\n==================================================================================================\n\n\nTotal params: 12,997,122\n\n\nTrainable params: 12,997,122\n\n\nNon-trainable params: 0\n\n\n__________________________________________________________________________________________________\n\n\nAgora é só compilar e ajustar:\n\nauc = keras.metrics.AUC(curve=\"ROC\")\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\", auc])\nmodel.fit(x=(p1, p2), y=y, epochs=5, batch_size=32, validation_split=0.2, verbose=1)"
  },
  {
    "objectID": "17-enem.html",
    "href": "17-enem.html",
    "title": "Enem",
    "section": "",
    "text": "import pandas as pd\nfrom tensorflow import keras\n\nCarregando os dados:\n\ndf = pd.read_csv(\"https://storage.googleapis.com/deep-learning-com-r/banco_de_questoes.csv\",encoding = 'latin2', sep = \";\")\ndf\n\n\n\n\n  \n    \n      \n      enunciados\n      url\n      grande_tema\n      subtema\n    \n  \n  \n    \n      0\n      Uma pessoa precisa comprar 15 sacos de cimento...\n      https://www.projetoagathaedu.com.br/questoes-e...\n      matematica\n      aritmetica-1\n    \n    \n      1\n      Antônio, Joaquim e José săo sócios de uma empr...\n      https://www.projetoagathaedu.com.br/questoes-e...\n      matematica\n      aritmetica-1\n    \n    \n      2\n      De acordo com pesquisas recentes, a expectativ...\n      https://www.projetoagathaedu.com.br/questoes-e...\n      matematica\n      aritmetica-1\n    \n    \n      3\n      Um jogo pedagógico é formado por cartas nas qu...\n      https://www.projetoagathaedu.com.br/questoes-e...\n      matematica\n      aritmetica-1\n    \n    \n      4\n      Em um país, as infraçőes de trânsito săo class...\n      https://www.projetoagathaedu.com.br/questoes-e...\n      matematica\n      aritmetica-1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      17338\n      Ao final do texto, a autora expőe um posiciona...\n      https://www.projetoagathaedu.com.br/prova-comp...\n      uerj\n      linguagens-2021\n    \n    \n      17339\n      As questőes 08 a 11 referem-se ao romance Tri...\n      https://www.projetoagathaedu.com.br/prova-comp...\n      uerj\n      linguagens-2021\n    \n    \n      17340\n      5. As questőes 08 a 11 referem-se ao romance ...\n      https://www.projetoagathaedu.com.br/prova-comp...\n      uerj\n      linguagens-2021\n    \n    \n      17341\n      As questőes 08 a 11 referem-se ao romance Tri...\n      https://www.projetoagathaedu.com.br/prova-comp...\n      uerj\n      linguagens-2021\n    \n    \n      17342\n      As questőes 08 a 11 referem-se ao romance Tri...\n      https://www.projetoagathaedu.com.br/prova-comp...\n      uerj\n      linguagens-2021\n    \n  \n\n17343 rows × 4 columns\n\n\n\nVamos separar a resposta e dados de treino\n\ndf.grande_tema.value_counts()\ny = df.grande_tema.astype('category').cat.codes.astype(\"int32\").to_numpy()\nx = df.enunciados.astype(\"str\").to_numpy()\n\nAgora vamos criar a camada de vetorização:\n\nvectorize = keras.layers.TextVectorization(\n  max_tokens = 10000,\n  pad_to_max_tokens=True\n)\nvectorize.adapt(x)\nvocab = vectorize.get_vocabulary()\n\nEntão podemos definir o modelo\n\ninput = keras.layers.Input(shape=(), dtype=\"string\")\noutput = vectorize(input)\noutput = keras.layers.Embedding(input_dim=len(vocab), output_dim=256)(output)\noutput = keras.layers.LSTM(units=256)(output)\noutput = keras.layers.Dense(units=256, activation=\"relu\")(output)\noutput = keras.layers.Dense(units=y.max() + 1, activation=\"softmax\")(output)\n\nCompilar e ajustar\n\nmodel = keras.Model(inputs=input, outputs=output)\nauc = keras.metrics.AUC(curve=\"ROC\")\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[auc, \"accuracy\"])\nmodel.summary()\n\nModel: \"model_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n input_2 (InputLayer)        [(None,)]                 0         \n\n\n                                                                 \n\n\n text_vectorization_1 (TextV  (None, None)             0         \n\n\n ectorization)                                                   \n\n\n                                                                 \n\n\n embedding_1 (Embedding)     (None, None, 256)         2560000   \n\n\n                                                                 \n\n\n lstm_1 (LSTM)               (None, 256)               525312    \n\n\n                                                                 \n\n\n dense_2 (Dense)             (None, 256)               65792     \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 116)               29812     \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 3,180,916\n\n\nTrainable params: 3,180,916\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\nmodel.fit(\n  x=x, \n  y=keras.utils.to_categorical(y),\n  epochs = 1,\n  verbose = 2\n)\n\n\n\n542/542 - 549s - loss: 4.1006 - auc_1: 0.7647 - accuracy: 0.0834 - 549s/epoch - 1s/step\n\n\n\n\n<keras.callbacks.History at 0x7fbb71cf01f0>"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exemplos de Deep Learning",
    "section": "",
    "text": "Esse site contém códigos de exemplos de deep learning usando Python. O objetivo é ter estes exemplos com bastante comentários para que o leitor consiga seguí-los sem precisar assistir a aula."
  },
  {
    "objectID": "18-autoencoder.html",
    "href": "18-autoencoder.html",
    "title": "Autoencoder",
    "section": "",
    "text": "from tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nVamos obter o banco de dados:\n\n(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n\nPodemos visualizar algumas imagens com:\n\nplt.imshow(x_train[0])\nplt.show()\nplt.imshow(x_train[1])\nplt.show()\nplt.imshow(x_train[2])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgora vamos definir o modelo. Nesse exemplo vamos usar uma forma um pouco diferente de definir o modelo que é usando classes. Dessa forma, podemos encapsular um pouco melhor o código. Também usaremos a keras.Sequential pela primeira vez, por isso vamos entender o que ele faz.\nOs 3 modelos definidos a seguir são equivalentes:\n\n# Usando a API funcional\ninput = keras.layers.Input(shape=(28,28,1))\noutput = keras.layers.Flatten()(input)\noutput = keras.layers.Dense(units=64, activation=\"relu\")(output)\nmodel = keras.Model(inputs=input, outputs=output)\n\n# Usando a API sequencial\nmodel = keras.Sequential([\n  keras.layers.Flatten(),\n  keras.layers.Dense(units=64, activation='relu'),\n])\n\n# usando a API de classes\nclass Modelo(keras.Model):\n  def __init__(self):\n    super(Modelo, self).__init__()\n    self.flatten = keras.layers.Flatten()\n    self.dense = keras.layers.Dense(units=64, activation=\"relu\")\n  def call(self, x):\n    out = self.flattent(x)\n    return self.dense(out)\n\nA vantagem da API sequencial do Keras é ser menos verbosa. No entanto, didaticamente a API funcional é mais clara, pois ela nos força a pensar em qual é o nosso input e como as camadas são combinadas. Já a API de classes é útil para modelos mais complicados pois permite que possamos encapsular melhor os códigos. Note que podemos misturar todas as API’s no mesmo código e isso é considerado idiomático.\nAgora vamos definir o autoencoder. Definimos ele em duas partes:\n\nUm encoder, que transforma a imagem do MNIST em um vetor.\nUm decoder, que pega um vetor e transforma de volta em uma imagem.\n\n\nclass Autoencoder(keras.Model):\n  def __init__(self, latent_dim):\n    super(Autoencoder, self).__init__()\n    self.latent_dim = latent_dim   \n    self.encoder = keras.Sequential([\n      keras.layers.Rescaling(scale=1./255),\n      keras.layers.Flatten(),\n      keras.layers.Dense(latent_dim, activation='relu'),\n    ])\n    self.decoder = keras.Sequential([\n      keras.layers.Dense(784, activation='sigmoid'),\n      keras.layers.Reshape((28, 28, 1)),\n      keras.layers.Rescaling(scale=255.)\n    ])\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nPara incializar um modelo definido como uma classe usamos:\n\nautoencoder = Autoencoder(latent_dim=64)\n\nEntão podemos compilar:\n\nautoencoder.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n\nE ajustar:\n\nautoencoder.fit(x_train, x_train,\n  epochs=10,\n  shuffle=True,\n  validation_data=(x_test, x_test),\n  verbose=2\n  )\n\nEpoch 1/10\n\n\n1875/1875 - 2s - loss: 1555.8877 - val_loss: 605.1500 - 2s/epoch - 1ms/step\n\n\nEpoch 2/10\n\n\n1875/1875 - 2s - loss: 452.4213 - val_loss: 349.3282 - 2s/epoch - 948us/step\n\n\nEpoch 3/10\n\n\n1875/1875 - 2s - loss: 329.5283 - val_loss: 296.8387 - 2s/epoch - 969us/step\n\n\nEpoch 4/10\n\n\n1875/1875 - 2s - loss: 298.9903 - val_loss: 277.7961 - 2s/epoch - 958us/step\n\n\nEpoch 5/10\n\n\n1875/1875 - 2s - loss: 286.0212 - val_loss: 278.3241 - 2s/epoch - 943us/step\n\n\nEpoch 6/10\n\n\n1875/1875 - 2s - loss: 278.3516 - val_loss: 268.9713 - 2s/epoch - 932us/step\n\n\nEpoch 7/10\n\n\n1875/1875 - 2s - loss: 271.6905 - val_loss: 264.3178 - 2s/epoch - 924us/step\n\n\nEpoch 8/10\n\n\n1875/1875 - 2s - loss: 266.9784 - val_loss: 256.6068 - 2s/epoch - 962us/step\n\n\nEpoch 9/10\n\n\n1875/1875 - 2s - loss: 262.8771 - val_loss: 257.2297 - 2s/epoch - 1ms/step\n\n\nEpoch 10/10\n\n\n1875/1875 - 2s - loss: 259.5267 - val_loss: 254.3345 - 2s/epoch - 1ms/step\n\n\n<keras.callbacks.History at 0x7fe645536e50>\n\n\nPodemos usar o autoencoder para simplificar e depois reconstruir uma imagem:\n\nencoded = autoencoder.encoder.predict(x_test)\ndecoded = autoencoder.decoder.predict(encoded)\n\nO ‘encoder’ do nosso autoencoder representa cada imagem como um vetor e esse vetor pode ser usado para comparar imagens, ou para ser input de algum outro modelo.\n\nencoded[0]\n\narray([ 2.7154648,  3.3469846,  5.0955553,  4.928041 ,  1.4984177,\n        3.5500624,  6.6044536,  4.8169136,  1.8987292,  3.7721162,\n        0.       , 10.673604 ,  3.438078 ,  4.828475 ,  3.6719105,\n        3.3354235,  3.8524048,  6.96551  ,  5.3519597,  6.073485 ,\n        1.730966 ,  3.6241064,  4.7289557,  5.14427  ,  5.7000866,\n        7.9746943,  4.687333 ,  3.6276262,  4.513113 ,  2.5262182,\n        2.7582881,  4.5068583,  5.326924 ,  1.5850337,  2.8732448,\n        2.304493 ,  5.060994 ,  5.387458 ,  0.7613592,  1.4485734,\n        0.       ,  2.7385345,  3.6030219, 13.330053 ,  2.6578474,\n        4.0867414,  4.838855 ,  4.833501 ,  3.9582207,  0.       ,\n        3.4030182,  2.8294477,  5.486768 ,  1.9036677,  3.8831449,\n        3.3715608,  3.5314097,  1.7339928,  3.1415982,  5.773363 ,\n        5.608508 ,  1.4208441,  4.4426064,  2.196546 ], dtype=float32)\n\n\nVisualize a imagem antes e depois:\n\nplt.imshow(x_test[0])\nplt.show()\nplt.imshow(decoded[0].astype(\"uint8\"))\nplt.show()"
  }
]