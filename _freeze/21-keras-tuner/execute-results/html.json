{
  "hash": "edc5fce6bf0704f0db915bd1180fb30c",
  "result": {
    "markdown": "---\ntitle: KerasTuner\n---\n\nNesse exemplo vamos usar o KerasTuner para encontrar melhores hiperparâmetros para um modelo.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom tensorflow import keras\nimport keras_tuner\nimport matplotlib.pyplot as pltX\n```\n:::\n\n\nVamos tentar encontrar qual o tamanho das convoluções que dá o melhor resultado para\na base do CIFAR10.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n(x, y), (x_test, y_test) = keras.datasets.cifar10.load_data()\n```\n:::\n\n\nCriando a classe para fazer o modelo, assim como no exemplo 19.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass Modelo(keras.Model):\n  def __init__(self, filters):\n    super(Modelo, self).__init__()\n    self.encoder = keras.Sequential([\n      keras.layers.Rescaling(1./255),\n      keras.layers.Conv2D(filters=filters, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n      keras.layers.MaxPool2D(pool_size=(2,2)),\n      keras.layers.Conv2D(filters=filters, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n      keras.layers.MaxPool2D(pool_size=(2,2)),\n      keras.layers.Conv2D(filters=filters, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n      keras.layers.MaxPool2D(pool_size=(2,2)),\n      keras.layers.Flatten()\n    ])\n    self.classifier = keras.Sequential([\n      keras.layers.Dense(128, activation=\"relu\"),\n      keras.layers.Dense(10, activation=\"softmax\")\n    ])\n  def call(self, x):\n    out = self.encoder(x)\n    return self.classifier(out)\n```\n:::\n\n\nAgora vamos definir uma função que inicializa o modelo:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef build_model(hp):\n  model = Modelo(\n    filters=hp.Int(\"filters\", min_value=8, max_value=32, step=4)\n  )\n  model.compile(\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    optimizer = \"adam\",\n    metrics = [\"accuracy\"]\n  )\n  return model\n```\n:::\n\n\nAgora vamos criar um objeto que controla a tunagem do modelo:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntuner = keras_tuner.RandomSearch(\n    hypermodel=build_model,\n    objective=\"val_accuracy\",\n    max_trials=3,\n    overwrite=True,\n    directory=\"tuning\",\n    project_name=\"cifar10\",\n)\ntuner.search_space_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSearch space summary\nDefault search space size: 1\nfilters (Int)\n{'default': None, 'conditions': [], 'min_value': 8, 'max_value': 32, 'step': 4, 'sampling': None}\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n2022-05-03 21:08:27.851920: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\nE por fim, podemos iniciar a tunagem:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx_train = x[:-10000]\nx_val = x[-10000:]\ny_train = y[:-10000]\ny_val = y[-10000:]\ntuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val), verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrial 3 Complete [00h 00m 34s]\nval_accuracy: 0.5178999900817871\n\nBest val_accuracy So Far: 0.6018999814987183\nTotal elapsed time: 00h 01m 51s\nINFO:tensorflow:Oracle triggered exit\n```\n:::\n:::\n\n\nAgora vemos a lista de modelos que ficaram melhor:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntuner.results_summary(num_trials=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResults summary\nResults in tuning/cifar10\nShowing 3 best trials\n<keras_tuner.engine.objective.Objective object at 0x7fe81c2245e0>\nTrial summary\nHyperparameters:\nfilters: 24\nScore: 0.6018999814987183\nTrial summary\nHyperparameters:\nfilters: 12\nScore: 0.5354999899864197\nTrial summary\nHyperparameters:\nfilters: 16\nScore: 0.5178999900817871\n```\n:::\n:::\n\n\nNo fim, podemos ajustar o modelo final:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nbest_hps = tuner.get_best_hyperparameters(3)\nmodel = build_model(best_hps[0])\nmodel.fit(x=x, y=y, epochs=2, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1563/1563 - 26s - loss: 1.5219 - accuracy: 0.4486 - 26s/epoch - 16ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/2\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1563/1563 - 24s - loss: 1.1703 - accuracy: 0.5836 - 24s/epoch - 16ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<keras.callbacks.History at 0x7fe81330cc40>\n```\n:::\n:::\n\n\nE avaliar o resultado na base de teste:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmodel.evaluate(x_test, y_test, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 1s - loss: 1.0989 - accuracy: 0.6028 - 1s/epoch - 4ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n[1.0989463329315186, 0.6028000116348267]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "21-keras-tuner_files"
    ],
    "filters": [],
    "includes": {}
  }
}