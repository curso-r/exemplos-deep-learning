{
  "hash": "c3e352d2c999db7ade15d26ea0308cc4",
  "result": {
    "markdown": "---\ntitle: Stochastic gradient descent\n---\n\n:::{.callout-note}\nEsse exemplo é muito parecido com o [exemplo 1](./01-linear-regression.html) com apenas modificações no loop de ajuste do modelo para usar o método da descida do gradiente estocástico - *stochastic gradient descent* ou também **SGD**.\n:::\n\nO objetivo desse exemplo é simular alguns dados e ajustar um modelo\nde regressão linear usando o método **estocástico** da descida do gradiente.\nImportante notar que queremos implementar o método da descida do gradiente de forma manual, isto é, sem usar bibliotecas que calculam \na derivada automaticamente.\n\n:::{.callout-note}\nA vantagem do SGD com relação ao GD é o custom computacional. O primeiro precisa\ncalcular derivadas de todas as observações, calcular o valor predito para todas as\nobservações, etc para cada passo. No **SGD**, como fazemos uma observação de cada vez,\nchegamos mais rápido no mínimo.\n\nOutra vantagem é o uso da memória. Em deep learning muitas vezes não conseguimos\ncarregar todos os dados de uma vez para a memória do computador. No caso do SGD,\né trivial implementá-lo sem precisar de todas as observações de uma vez.\n\nAlém disso, é estudado que o SGD tem um efeito de regularização no ajuste do modelo.\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\nEm primeiro lugar simulamos alguns dados. `x` vai ser uma variável contendo `n` valores aleatórios obtidos a partir da distribuição uniforme.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn = 1000\nx = np.random.uniform(size=(n,))\n```\n:::\n\n\nEm seguida definimos os pesos `W` e `b` para a simulação. Nosso objetivo depois, vai ser *fingir* que não sabemos esses valores e ajustar um modelo para encontrá-los.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nW = 0.9\nb = 0.1\n```\n:::\n\n\nDefinimos então a variável resposta:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ny = W * x + b\n```\n:::\n\n\nAgora vamos definir o nosso modelo. Ele é uma classe do python que recebe os valores iniciais dos pesos `w` e `b` na inicialização e possui o método `predict` que\nfaz uma transformação linear `w*x + b` no input `x`.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass Model:\n    def __init__(self, w, b):\n        self.w = w\n        self.b = b\n\n    def predict(self, x):\n        return self.w * x + self.b\n```\n:::\n\n\nAgora vamos definir a função de perda. Nesse caso estamos usando o erro quadrático médio:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef loss(y_true, y_hat):\n    return np.mean((y_true - y_hat) ** 2)\n```\n:::\n\n\nTambém definimos as derivadas da função de perda `l` em relação a cada uma das partes do grafo de computação até chegar nos parâmetros que queremos estimar.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# derivada da função de perda em relação à y_hat\ndef dl_dyhat(y_hat, x, y):\n    return 2 * (y - y_hat) * (-1)\n\n# derivada de y_hat com relação a w\ndef dyhat_dw(y_hat, x, y):\n    return x\n\n# derivada de y_hat com relação a b (viés)\ndef dyhat_db(y_hat, x, y):\n    return 1.0\n```\n:::\n\n\nAgora vamos inicializar o modelo:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nw = np.random.uniform(size=1)\nb = 0.0\nmodel = Model(w, b)\n```\n:::\n\n\nE podemos, finalmente, implementar o método da descida do gradiente.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nlr = 0.1 # learning rate - qual o tamanho do passo que damos em cada iteração.\nnum_epochs = 2 # o número de épocas é o número de vezes que passamos pela base inteira.\nindex = np.arange(x.size)\n\nfor epoch in range(num_epochs):\n    np.random.shuffle(index)\n    for i in list(index):\n        y_hat = model.predict(x[i])\n        model.w -= lr * np.mean(\n            dl_dyhat(y_hat, x[i], y[i]) * dyhat_dw(y_hat, x[i], y[i])\n        )\n        model.b -= lr * np.mean(\n            dl_dyhat(y_hat, x[i], y[i]) * dyhat_db(y_hat, x[i], y[i])\n        )\n    print(\"epoch:\", epoch, \"loss:\", loss(y, model.predict(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nepoch: 0 loss: 4.285631346360619e-15\nepoch: 1 loss: 3.5306078809548166e-27\n```\n:::\n:::\n\n\nVerificamos que o resultado está conforme o esperado. Isto é, `w` e `b` estão\nparecidos com os valores usamos para gerar os dados.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nprint(model.w)\nprint(model.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.9]\n0.10000000000011379\n```\n:::\n:::\n\n\n",
    "supporting": [
      "02-sgd_files"
    ],
    "filters": [],
    "includes": {}
  }
}