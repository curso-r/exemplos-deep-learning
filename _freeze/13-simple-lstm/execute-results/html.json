{
  "hash": "febc7f5e34534511da2be5e11571641a",
  "result": {
    "markdown": "---\ntitle: LSTM's\n---\n\nNeste exemplo, vamos demonstrar as contas como são feitas para a LSTM, ao invés de RNN's como fizemos no [exemplo anterior](./12-simple-rnn.qmd).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom tensorflow import keras\n```\n:::\n\n\nPrimeiro, vamos gerar os dados novamente:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn = 10000\nl = 10\n\ncresc = np.random.randint(0, 2, size=(n,))\nx = np.empty((n, l), dtype=np.float32)\nfor i, cr in enumerate(cresc):\n    tmp = np.random.uniform(size=(l,))\n    if cr == 1:\n        x[i, :] = tmp[np.argsort(tmp)]\n    else:\n        x[i, :] = tmp[np.argsort(-tmp)]\nx = x.reshape((n, l, 1))\n```\n:::\n\n\nAgora vamos definir o modelo no Keras. Não vamos nos preocupar com os\nparâmetros nem nada ainda. A seguir vamos mostrar exatamente as contas que o Keras está fazendo por trás.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ninput = keras.layers.Input(shape=(l, 1))\noutput = keras.layers.LSTM(units=1, activation=\"tanh\", use_bias=False, recurrent_activation=\"sigmoid\")(input)\noutput = keras.layers.Activation(\"sigmoid\")(output)\nmodel = keras.Model(inputs=input, outputs=output)\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=\"accuracy\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2022-05-01 15:00:16.590139: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\nAgora ajustamos o modelo:\n\n::: {.cell .column-screen-right layout-ncol='1' execution_count=4}\n``` {.python .cell-code}\nmodel.fit(x=x, y=cresc, epochs=10, batch_size=32, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 1s - loss: 0.5692 - accuracy: 0.5122 - 1s/epoch - 4ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5240 - accuracy: 0.5884 - 482ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5175 - accuracy: 0.6381 - 488ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 4/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5116 - accuracy: 0.6825 - 489ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5062 - accuracy: 0.7285 - 488ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 6/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5030 - accuracy: 0.7500 - 483ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 7/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5005 - accuracy: 0.7698 - 488ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 8/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5011 - accuracy: 0.7617 - 484ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 9/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5017 - accuracy: 0.7678 - 488ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 10/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.5001 - accuracy: 0.7730 - 487ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<keras.callbacks.History at 0x7fee81e2ac70>\n```\n:::\n:::\n\n\nAgora vamos ver a conta linha por linha que é feita pelo Keras para cada\nobservação. Definimos as funções que vamos usar:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n```\n:::\n\n\nSelecionamos uma observação:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx_ = x[0]\n```\n:::\n\n\nObtemos os pesos estimados pelo Keras:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nw = model.get_weights()\n```\n:::\n\n\nInicializamos os estados `s` e `c` e dentro do loop escrevemos a regra de atualização dos estados:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ns = 0\nc = 0\n\n\nfor t in range(l):\n    f = sigmoid(w[1][0, 1] * s + w[0][0, 1] * x_[t])\n    i = sigmoid(w[1][0, 0] * s + w[0][0, 0] * x_[t])\n    c_hat = np.tanh(w[1][0, 2] * s + w[0][0, 2] * x_[t])\n    c = f * c + i * c_hat\n    o = sigmoid(s * w[1][0, 3] + w[0][0, 3] * x_[t])\n    s = o * np.tanh(c)\n```\n:::\n\n\nEmfim, aplicamos o sigmoid no estado final:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nsigmoid(s)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([0.73093957], dtype=float32)\n```\n:::\n:::\n\n\nO valor predito pelo keras para essa observação seria:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nmodel.predict(x)[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([0.7309395], dtype=float32)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "13-simple-lstm_files"
    ],
    "filters": [],
    "includes": {}
  }
}