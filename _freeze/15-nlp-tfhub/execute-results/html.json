{
  "hash": "9ef8f416d1fed871e347e81066ca6ad9",
  "result": {
    "markdown": "---\ntitle: Modelo pré-treinado\n---\n\nNesse exemplo vamos ajustar um modelo pré-treinado para a mesma base usando embeddings provenientes de um tipo de modelo que é bem avançado chamado BERT.\nPara isso vamos usar o tfhub.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom tensorflow import keras\nimport tensorflow_text\nimport tensorflow_hub as hub\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\nA tarefa será bem similar com o que fizemos quando usamos modelos pré treinados\npara imagens.\n\nEm primeiro lugar vamos obter o banco de dados:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndataset = pd.read_csv(\n    \"https://storage.googleapis.com/deep-learning-com-r/toxic-comments.csv\"\n)\ndataset.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nOs modelos pré-treinados do tfhub já incluem as suas camadas de pré-processamento, portanto não precisamos nos preocupar em pré-processar\nos textos usando a camada de vetorização. Eles também incluem os embeddings, portanto não vamos criar essas camadas. Muitos dos modelos também retornam embeddings completas para os textos, não apenas para as palavras. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nx = dataset[\"comment_text\"].to_numpy()\ny = dataset.iloc[:, 2:].to_numpy()\n```\n:::\n\n\nVamos definir o modelo no Keras:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ninput = keras.layers.Input(shape=(), dtype=\"string\")\nencoded = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")(\n    input\n)\nencoded = hub.KerasLayer(\n    \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\", trainable=False\n)(encoded)\n\noutput = keras.layers.Dense(units=y.shape[1], activation=\"sigmoid\")(encoded[\"default\"])\n```\n:::\n\n\nO modelo pode ser definido com:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nmodel = keras.Model(inputs=input, outputs=output)\n```\n:::\n\n\nAgora podemos ajustar o modelo. Basicamente só estamos ajustando classificador que se baseia nas embeddings criadas pelo BERT.\n\n::: {.cell .column-screen-right layout-ncol='1' execution_count=6}\n``` {.python .cell-code}\nauc = keras.metrics.AUC(curve=\"ROC\")\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\", auc])\n\nmodel.fit(x, y, epochs=1, batch_size=32, validation_split=0.2, verbose=1)\n```\n:::\n\n\n```\n3990/3990 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9733 - auc: 0.8957\n```\n\nO modelo demora para rodar. Mas em apenas uma época conseguimos um bom resultado.\n\n",
    "supporting": [
      "15-nlp-tfhub_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}