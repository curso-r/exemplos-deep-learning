{
  "hash": "11920c359d02169760ed7316c8a2d0fc",
  "result": {
    "markdown": "---\ntitle: Multi-Layer Perceptron\n---\n\nNesse exemplos ajustamos um multi-layer perceptron para dados simulados.\nA diferença para o que fizemos nos exemplos anteriores, é que fica mais fácil\nusar a notação matricial especificar as contas.\n\nO Keras já usa a notação matricial, por isso, podemos ver que o código no Keras\nmuda bem pouco.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom tensorflow import keras\n```\n:::\n\n\nVeja que nesse exemplo a geração dos dados é um pouco mais complicada. Usamos\na mesma idéia do MLP.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn = 1000\n\n# uma matriz com `n` observações e 2 colunas. \n# valores aleatórios obtidos da distribuição uniforme.\nX = np.random.uniform(size=(n, 2))\n\n# função relu, usada comunmente como função de ativação em \n# redes neurais.\ndef relu(x):\n    return x * (x > 0)\n\n# Dois conjuntos de pesos: W e B para a primeira camada, V e a para a segunda.\nW = np.array([[0.2, 0.7], [-0.4, 0.5]])\nB = np.array([0.1, -0.2])\nV = np.array([[0.6], [0.7]])\na = 0.9\n\n# O y é gerado dessa forma\n# `np.dot()` é a mesma coisa que multiplição de matrizes.\ny = relu(np.dot(X, W) + B)\ny = np.dot(y, V) + a\n```\n:::\n\n\nAgora que os dados foram gerados vamos definir o modelo. Pontos importantes:\n\n1. A camada de input agora tem shape `(2,)`, isto é o input é uma matriz com `n`\nlinhas e **`2`** colunas.\n2. A camada *escondida* (hidden layer) do modelo possui `1000` unidades (ou neurônios).\nA função de ativação é `relu`. É muito importante que as camadas intermediárias do MLP\npossuam uma função de ativação.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ninput = keras.layers.Input(shape=(2,))\nhidden = keras.layers.Dense(units=1000, activation=\"relu\")(input)\noutput = keras.layers.Dense(units=1)(hidden)\n\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model_1\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n input_2 (InputLayer)        [(None, 2)]               0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_2 (Dense)             (None, 1000)              3000      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_3 (Dense)             (None, 1)                 1001      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 4,001\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 4,001\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\nA quantidade de parâmetros da primeira camada é dada por:\n\n- 2 parâmetros para cada neurônio (um por coluna do input) = 2000\n- 1 viés para cada neurônio = 1000\n- **Total**: 3000 \n\nPara a camada de output temos fazemos:\n\n- 1000 parâmetros para cada neurônio = 1000\n- 1 viés para cada neurônios = 1\n- **Total** = 1001\n\nAgora da mesma forma que nos exemplos anteriores definimos a loss e o otimizador.\nVeja que o Keras também permite usar strings para definir esses atributos - `'mse'` é\num atalho para `keras.losses.mean_squared_error`.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\n```\n:::\n\n\nFinalmente podemos ajustar o modelo:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nhistory = model.fit(x=X, y=y, batch_size=10, epochs=5, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n100/100 - 0s - loss: 0.1332 - 206ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n100/100 - 0s - loss: 0.0028 - 55ms/epoch - 549us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n100/100 - 0s - loss: 0.0018 - 52ms/epoch - 519us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 4/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n100/100 - 0s - loss: 0.0013 - 53ms/epoch - 530us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n100/100 - 0s - loss: 0.0010 - 52ms/epoch - 523us/step\n```\n:::\n:::\n\n\n",
    "supporting": [
      "04-mlp_files"
    ],
    "filters": [],
    "includes": {}
  }
}