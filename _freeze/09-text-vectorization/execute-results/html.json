{
  "hash": "76c1cc6b87fdef1b30ea117c115ffa1b",
  "result": {
    "markdown": "---\ntitle: Pré-processamento\n---\n\nNesse exemplo fazemos o primeiro passo do pré-processamento para modelos de texto\nem deep leanring que é transformar cada texto em uma sequência de palavras em \numa sequência de números inteiros.\n\nPara isso, usamos a camada `TextVectorization`. Note que dsta camada é um pouco\ndiferente das demais, pois, precisamos 'adaptá-la' antes de usar.\n\nO parâmetro `max_tokens` diz o número máximo de palavras que a camada vai guardar\nno seu vocabulário. Isso é usado quando não queremos que palavras que aparecem\nmuito pouco tenham um número inteiro atribuido.\n\nO `output_mode='int'` indica que queremos transformar cada palavra em um número inteiro.\nExistem outras formas de vetorizar (por exemplo tf-idf) mas não vamos usá-las em\ndeep learning.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom tensorflow import keras\nlayer = keras.layers.TextVectorization(max_tokens=10, output_mode=\"int\",)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2022-05-01 10:30:26.275255: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\nAgora vamos adaptar a camada para um conjunto de frases:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrases = [\n    \"eu gosto de gatos\",\n    \"eu gosto de cachorros\",\n    \"eu gosto de gatos e cachorros\",\n]\n\nlayer.adapt(frases)\n```\n:::\n\n\nEntão podemos usá-la:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nlayer(frases)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<tf.Tensor: shape=(3, 6), dtype=int64, numpy=\narray([[3, 2, 4, 5, 0, 0],\n       [3, 2, 4, 6, 0, 0],\n       [3, 2, 4, 5, 7, 6]])>\n```\n:::\n:::\n\n\nE obter o vocabulário:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nvocab = layer.get_vocabulary()\nprint(vocab)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['', '[UNK]', 'gosto', 'eu', 'de', 'gatos', 'cachorros', 'e']\n```\n:::\n:::\n\n\nNote que no vocabulário, sempre temos que o primeiro ítem é `''` e o segundo é\n`<UNK>`. Esses tokens servem para padding e para palavras desconhecidas pela camada\nde vetorização respectivamente.\n\nPor exemplo, se fizermos:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nlayer(\"eu gosto de tubarão\")\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 2, 4, 1])>\n```\n:::\n:::\n\n\nVeja que a palavra 'tubarao' foi substituida pelo inteiro representado o `<UNK>` pois\nessa palavra não aparecia quando adaptamos a camada.\n\n",
    "supporting": [
      "09-text-vectorization_files"
    ],
    "filters": [],
    "includes": {}
  }
}