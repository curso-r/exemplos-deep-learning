{
  "hash": "2dd5b217c119c4363b8bd5f0a2e71108",
  "result": {
    "markdown": "---\ntitle: Pooling\n---\n\nNesse exemplo vamos usar as camadas de TextVectorization do [exemplo 09](./09-text-vectorization.qmd) e a camada de embedding do [exemplo anterior](./10-embedding.qmd) para transformar cada palavra em um vetor com 1 elemento. \n\nEm seguida vamos considerar que cada texto (no caso do banco de dados, comentários feiros em forum da internet) pode ser representado pela média\ndas palavras que têm nele. Usar a média é também chamado de `GlobalAveragePooling` no Keras:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom tensorflow import keras\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n```\n:::\n\n\nVamos carregar o banco de dados:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndataset = pd.read_csv(\n    \"https://storage.googleapis.com/deep-learning-com-r/toxic-comments.csv\"\n)\ndataset.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNosso objetivo será, a partir do texto do comentário, classificar em tóxico ou não tóxico.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nx = dataset['comment_text'].to_numpy()\ny = dataset.iloc[:,2].to_numpy()\n```\n:::\n\n\nPara escolher o valor de `output_sequence_length`, isto é, o número máximo de palavras\nem cada texto, fazemos um histograma do número de palavras por texto:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnumero_palavras = [len(i.split()) for i in x]\nsns.histplot(numero_palavras)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n<AxesSubplot:ylabel='Count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](11-avg-pooling_files/figure-html/cell-5-output-2.png){width=602 height=404}\n:::\n:::\n\n\nComo a maioria dos textos possui menos do que 150 palavras, escolhemos que o \ntamanho máximo é 150. Dessa forma, se um texto for muito grande, usamos apenas\nas primeiras 150 palavras para classificar em tóxico/não tóxico. Isso vai permitir\nque o modelo fique muito mais rápido.\n\nVamos então criar a camada de vetorização e adaptá-la:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nvectorize = keras.layers.TextVectorization(\n    max_tokens=10000, output_mode=\"int\", output_sequence_length=150\n)\n\nvectorize.adapt(x)\nvocab = vectorize.get_vocabulary()\n```\n:::\n\n\nAgora vamos definir o modelo. Lembre que cada palavra será representada por um vetor\npela camada de embedding e queremos que a média desses valores represente a probabilidade\ndo texto ser tóxicou ou não.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ninput = keras.layers.Input(shape=(), dtype=\"string\")\noutput = vectorize(input)\noutput = keras.layers.Embedding(input_dim=len(vocab), output_dim=1)(output)\noutput = keras.layers.GlobalAveragePooling1D()(output)\noutput = keras.layers.Activation(\"sigmoid\")(output)\n\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model_1\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n input_2 (InputLayer)        [(None,)]                 0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text_vectorization_1 (TextV  (None, 150)              0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n ectorization)                                                   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n embedding_1 (Embedding)     (None, 150, 1)            10000     \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n global_average_pooling1d_1   (None, 1)                0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n (GlobalAveragePooling1D)                                        \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n activation_1 (Activation)   (None, 1)                 0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 10,000\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 10,000\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\nNesse modelo, o número de parâmetros é igual ao número de palavras que temos no\nvocabulário.\n\nAgora podemos compilar o modelo. Vamos usar a 'binary_crossentropy' pois o problema\né de classificação binária. Além da acurácia, vamos medir o AUC, uma vez que o problema\né bem desbalanceado (muitos zeros e poucos uns).\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nauc = keras.metrics.AUC(curve=\"ROC\")\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", auc])\n```\n:::\n\n\nAgora vamos ajustar o modelo:\n\n::: {.cell .column-screen-right layout-ncol='1' execution_count=8}\n``` {.python .cell-code}\nmodel.fit(x, y, epochs=5, batch_size=32, validation_split=0.2, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n3990/3990 - 6s - loss: 0.4087 - accuracy: 0.9041 - auc_1: 0.5017 - val_loss: 0.3155 - val_accuracy: 0.9051 - val_auc_1: 0.5966 - 6s/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n3990/3990 - 5s - loss: 0.3060 - accuracy: 0.9044 - auc_1: 0.7704 - val_loss: 0.2966 - val_accuracy: 0.9053 - val_auc_1: 0.8276 - 5s/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n3990/3990 - 6s - loss: 0.2939 - accuracy: 0.9045 - auc_1: 0.8131 - val_loss: 0.2884 - val_accuracy: 0.9055 - val_auc_1: 0.8167 - 6s/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 4/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n3990/3990 - 6s - loss: 0.2871 - accuracy: 0.9047 - auc_1: 0.8222 - val_loss: 0.2826 - val_accuracy: 0.9057 - val_auc_1: 0.8313 - 6s/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n3990/3990 - 7s - loss: 0.2816 - accuracy: 0.9049 - auc_1: 0.8385 - val_loss: 0.2776 - val_accuracy: 0.9058 - val_auc_1: 0.8462 - 7s/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n<keras.callbacks.History at 0x7f9c47499e50>\n```\n:::\n:::\n\n\nPodemos indetificar as palavras com os maiores e menores valores de embedding.\nComo o texto é classificado em tóxico/não tóxico com base na média dos valores\ndas palavras dele, palavras com valores altos ajudam a classificar o texto como \ntóxico e palavras com valores baixos ajudam a considerar o texto como não tóxico.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nembeddings = model.get_weights()[1]\nwords = pd.DataFrame.from_dict({\n  \"word\": vocab,\n  \"embedding\": embeddings[:,0]\n})\n```\n:::\n\n\nAs 10 palavras com maires valores de embeddings são:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nwords.sort_values(\"embedding\", ascending = False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>382</th>\n      <td>fucking</td>\n      <td>8.187375</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>fuck</td>\n      <td>7.190009</td>\n    </tr>\n    <tr>\n      <th>563</th>\n      <td>stupid</td>\n      <td>6.245700</td>\n    </tr>\n    <tr>\n      <th>318</th>\n      <td>shit</td>\n      <td>5.337469</td>\n    </tr>\n    <tr>\n      <th>1190</th>\n      <td>idiot</td>\n      <td>5.320159</td>\n    </tr>\n    <tr>\n      <th>762</th>\n      <td>bitch</td>\n      <td>5.001073</td>\n    </tr>\n    <tr>\n      <th>741</th>\n      <td>hell</td>\n      <td>4.981386</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>off</td>\n      <td>4.903869</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>you</td>\n      <td>4.426858</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>life</td>\n      <td>4.257237</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nE as 10 palavras com menores valores são:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nwords.sort_values(\"embedding\", ascending = True).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24</th>\n      <td>article</td>\n      <td>-9.915396</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>thank</td>\n      <td>-9.775599</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>please</td>\n      <td>-9.449037</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the</td>\n      <td>-9.399350</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>may</td>\n      <td>-8.358996</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>for</td>\n      <td>-8.049402</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>as</td>\n      <td>-7.831261</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>welcome</td>\n      <td>-7.755289</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>thanks</td>\n      <td>-7.719523</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>be</td>\n      <td>-7.713208</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nVemos que os embeddings que são comportaram como imaginávamos. Maiores valores indicam\npalavras que levam à textos tóxicos e valores menores levam a textos não tóxicos.\n\n",
    "supporting": [
      "11-avg-pooling_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}