{
  "hash": "f64c561ac535119aaae2356f49a4639d",
  "result": {
    "markdown": "---\ntitle: Word embeddings\n---\n\nNesse exemplo vamos mostrar como usar a camada de embeddings. Antes de usá-la\nprecisamos transformar cada texto em uma sequência de inteiros, como vimos\nno [exemplo anterior](./09-text-vectorization.html).\n\nPara isso, usamos a camada `TextVectorization`. Note que dsta camada é um pouco\ndiferente das demais, pois, precisamos 'adaptá-la' antes de usar.\n\nO parâmetro `max_tokens` diz o número máximo de palavras que a camada vai guardar\nno seu vocabulário. Isso é usado quando não queremos que palavras que aparecem\nmuito pouco tenham um número inteiro atribuido.\n\nO `output_mode='int'` indica que queremos transformar cada palavra em um número inteiro.\nExistem outras formas de vetorizar (por exemplo tf-idf) mas não vamos usá-las em\ndeep learning.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom tensorflow import keras\nlayer = keras.layers.TextVectorization(max_tokens=10, output_mode=\"int\",)\n```\n:::\n\n\nAgora vamos adaptar a camada para um conjunto de frases:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrases = [\n    \"eu gosto de gatos\",\n    \"eu gosto de cachorros\",\n    \"eu gosto de gatos e cachorros\",\n]\n\nlayer.adapt(frases)\nvocab = layer.get_vocabulary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7feca7c26670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n```\n:::\n:::\n\n\nAgora podemos definir um input, passar a camada de vetorização e em seguida definir a\ncamada de embeddings.\nO parâmetro `input_dim` da camada `Embedding` indica o número de tokens possíveis.\nNo nosso caso, esse número é o tamanho do vocabulário.\nA camada de embedding faz cada palavra ser representada por um vetorizinho. O parâmetro\n`output_dim` indica o tamanho desse vetor. No exemplo, cada palavra será representada\npor um vetor de tamanho 2.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ninput = keras.layers.Input(shape=(), dtype=\"string\")\nvec = layer(input)\noutput = keras.layers.Embedding(input_dim=len(vocab), output_dim=2)(vec)\n```\n:::\n\n\nAgora podemos criar um modelo:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nmodel = keras.Model(inputs=input, outputs=output)\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model_3\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n input_5 (InputLayer)        [(None,)]                 0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text_vectorization_5 (TextV  (None, None)             0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n ectorization)                                                   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n embedding_4 (Embedding)     (None, None, 2)           16        \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 16\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 16\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\nA camada de embeddings possui `len(vocab)*2` parâmetros. Isto é, 2 parâmetros\npara cada palavra do vocabulário.\n\nAgora vamos ver como uma frase fica representada pela camada de embeddings:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nmodel.predict(frases)[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\narray([[ 0.04450825,  0.04365969],\n       [ 0.00506002, -0.00034119],\n       [-0.00431657, -0.03885051],\n       [ 0.04931844,  0.03272061],\n       [ 0.00938363, -0.02423037],\n       [ 0.00938363, -0.02423037]], dtype=float32)\n```\n:::\n:::\n\n\nTambém podemos observar a matriz de embeddings:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nmodel.get_weights()[1]\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\narray([[ 0.00938363, -0.02423037],\n       [-0.04308408,  0.00859753],\n       [ 0.00506002, -0.00034119],\n       [ 0.04450825,  0.04365969],\n       [-0.00431657, -0.03885051],\n       [ 0.04931844,  0.03272061],\n       [ 0.03986887,  0.01469808],\n       [-0.04277081,  0.01205132]], dtype=float32)\n```\n:::\n:::\n\n\nCada linha aqui é um vetor que representa uma palavra do vocabulário. A order é a mesma\nque aparece em `vocab`:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nprint(vocab)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['', '[UNK]', 'gosto', 'eu', 'de', 'gatos', 'cachorros', 'e']\n```\n:::\n:::\n\n\n",
    "supporting": [
      "10-embedding_files"
    ],
    "filters": [],
    "includes": {}
  }
}