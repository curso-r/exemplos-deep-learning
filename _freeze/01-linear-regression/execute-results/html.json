{
  "hash": "21d07f33729da3fc36d728b5798279a8",
  "result": {
    "markdown": "---\ntitle: Regressão Linear\n---\n\nO objetivo desse exemplo é simular alguns dados e ajustar um modelo\nde regressão linear usando o método da descida do gradiente.\nImportante notar que queremos implementar o método da descida do gradiente de forma manual, isto é, sem usar bibliotecas que calculam \na derivada automaticamente.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\nEm primeiro lugar simulamos alguns dados. `x` vai ser uma variável contendo `n` valores aleatórios obtidos a partir da distribuição uniforme.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn = 1000\nx = np.random.uniform(size=(n,))\n```\n:::\n\n\nEm seguida definimos os pesos `W` e `b` para a simulação. Nosso objetivo depois, vai ser *fingir* que não sabemos esses valores e ajustar um modelo para encontrá-los.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nW = 0.9\nb = 0.1\n```\n:::\n\n\nDefinimos então a variável resposta:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ny = W * x + b\n```\n:::\n\n\nAgora vamos definir o nosso modelo. Ele é uma classe do python que recebe os valores iniciais dos pesos `w` e `b` na inicialização e possui o método `predict` que\nfaz uma transformação linear `w*x + b` no input `x`.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass Model:\n    def __init__(self, w, b):\n        self.w = w\n        self.b = b\n\n    def predict(self, x):\n        return self.w * x + self.b\n```\n:::\n\n\nAgora vamos definir a função de perda. Nesse caso estamos usando o erro quadrático médio:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef loss(y_true, y_hat):\n    return np.mean((y_true - y_hat) ** 2)\n```\n:::\n\n\nTambém definimos as derivadas da função de perda `l` em relação a cada uma das partes do grafo de computação até chegar nos parâmetros que queremos estimar.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# derivada da função de perda em relação à y_hat\ndef dl_dyhat(y_hat, x, y):\n    return 2 * (y - y_hat) * (-1)\n\n# derivada de y_hat com relação a w\ndef dyhat_dw(y_hat, x, y):\n    return x\n\n# derivada de y_hat com relação a b (viés)\ndef dyhat_db(y_hat, x, y):\n    return 1.0\n```\n:::\n\n\nAgora vamos inicializar o modelo:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nw = np.random.uniform(size=1)\nb = 0.0\nmodel = Model(w, b)\n```\n:::\n\n\nE podemos, finalmente, implementar o método da descida do gradiente.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nlr = 0.1 # learning rate - qual o tamanho do passo que damos em cada iteração.\nfor step in range(500):\n    y_hat = model.predict(x)\n    dldyhat = dl_dyhat(y_hat, x, y)\n    model.w -= lr * np.mean(dldyhat * dyhat_dw(y_hat, x, y))\n    model.b -= lr * np.mean(dldyhat * dyhat_db(y_hat, x, y))\n    if (step % 100) == 0:\n        print(\"step:\", step, \"loss:\", loss(y, y_hat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nstep: 0 loss: 0.22998169398463467\nstep: 100 loss: 0.001345444126299616\nstep: 200 loss: 9.543931127460992e-05\nstep: 300 loss: 6.770004014676807e-06\nstep: 400 loss: 4.802314030416836e-07\n```\n:::\n:::\n\n\nVerificamos que o resultado está conforme o esperado. Isto é, `w` e `b` estão\nparecidos com os valores usamos para gerar os dados.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nprint(model.w)\nprint(model.b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.89936771]\n0.10034437382944898\n```\n:::\n:::\n\n\n",
    "supporting": [
      "01-linear-regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}