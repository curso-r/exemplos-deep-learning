{
  "hash": "5538c43bc1b7c4ff89ede554eb1b084b",
  "result": {
    "markdown": "---\ntitle: Data augmentation\n---\n\nNesse exemplo vamos mostrar o que é e para que serve técnicas de data augmentation.\nVamos mostrar no contexto de imagens, mas essa idéia pode valer para diversas \noutras aplicações.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nVamos usar o banco de dados do CIFAR10:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n```\n:::\n\n\nData augmentation consiste em usar heurísticas para modificar os dados de uma forma\nque poderia acontecer na vida real. Em imagens é muito comum mudar um pouco o brilho,\nfazer um pouquinho de zoom ou rotacionar um pouco a imagem.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata_augmentation = keras.Sequential([\n  keras.layers.RandomFlip(\"horizontal\"),\n  keras.layers.RandomRotation(0.2)\n])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2022-05-03 16:40:40.167394: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\nVamos ver o que acontece com as imagens depois dessa transformação:\n\n::: {.cell .column-screen-right layout-ncol='2' execution_count=4}\n``` {.python .cell-code}\nplt.imshow(x_train[0])\nplt.show()\nplt.imshow(data_augmentation(x_train[0:1]).numpy()[0].astype(\"uint8\"))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](19-data-augmentation_files/figure-html/cell-5-output-1.png){width=408 height=404}\n:::\n\n::: {.cell-output .cell-output-display}\n![](19-data-augmentation_files/figure-html/cell-5-output-2.png){width=408 height=404}\n:::\n:::\n\n\nFazer essas transformações pode melhorar bastante o acerto do modelo além de \najusar na generalização. É como se estivéssemos fornecendo um banco de dados\nbem maior do que o que possuimos.\n\nAgora podemos definir um modelo que usa essas transformações, ajustar e treinar.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass Modelo(keras.Model):\n  def __init__(self):\n    super(Modelo, self).__init__()\n    self.data_augmentation = keras.Sequential([\n      keras.layers.RandomFlip(\"horizontal\"),\n      keras.layers.RandomRotation(0.2)\n    ])\n    self.encoder = keras.Sequential([\n      keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n      keras.layers.MaxPool2D(pool_size=(2,2)),\n      keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n      keras.layers.MaxPool2D(pool_size=(2,2)),\n      keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n      keras.layers.MaxPool2D(pool_size=(2,2)),\n      keras.layers.Flatten()\n    ])\n    self.classifier = keras.Sequential([\n      keras.layers.Dense(128, activation=\"relu\"),\n      keras.layers.Dense(10, activation=\"softmax\")\n    ])\n  def call(self, x):\n    out = self.data_augmentation(x)\n    out = self.encoder(out)\n    return self.classifier(out)\n```\n:::\n\n\nPara compilar usamos:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nmodel = Modelo()\nmodel.compile(\n  loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n  optimizer = \"adam\",\n  metrics = [\"accuracy\"]\n)\n```\n:::\n\n\nE ajustamos com:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1563/1563 - 93s - loss: 1.9322 - accuracy: 0.3553 - val_loss: 1.6136 - val_accuracy: 0.4348 - 93s/epoch - 60ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1563/1563 - 90s - loss: 1.5953 - accuracy: 0.4297 - val_loss: 1.4945 - val_accuracy: 0.4607 - 90s/epoch - 58ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1563/1563 - 137s - loss: 1.5013 - accuracy: 0.4632 - val_loss: 1.3715 - val_accuracy: 0.5133 - 137s/epoch - 88ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 4/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1563/1563 - 122s - loss: 1.4333 - accuracy: 0.4908 - val_loss: 1.3586 - val_accuracy: 0.5198 - 122s/epoch - 78ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5/5\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1563/1563 - 101s - loss: 1.3687 - accuracy: 0.5101 - val_loss: 1.3509 - val_accuracy: 0.5299 - 101s/epoch - 64ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<keras.callbacks.History at 0x7f8b4ed34eb0>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "19-data-augmentation_files"
    ],
    "filters": [],
    "includes": {}
  }
}