{
  "hash": "9173e86b231857b20ba3026985e4b639",
  "result": {
    "markdown": "---\ntitle: Regressão Logística\n---\n\nNesse exemplos vamos ajustar um modelo de regressão logística para dados simulados.\nO precesso de simulação dos dados lembra o [exemplo 4](./04-mlp.html).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nfrom tensorflow import keras\nfrom sklearn import metrics\n```\n:::\n\n\nVamos gerar os dados de input:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn = 1000\n\nX = np.random.uniform(size=(n, 2))\nW = np.array([[-0.6], [0.7]])\nB = 0.1\n```\n:::\n\n\nAgora vamos gerar a variável resposta. Note que primeiro geramos a probabilidade\ndo `y` ser 1, usando `sigmoid(np.dot(X, W) + B)`. Em seguida, se a probabilidade\nde `y` ser igual 1 for maior do que 0.5, dizemos que ele vale 1.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ny = sigmoid(np.dot(X, W) + B)\nprint(\"Prob y=1\", y[0:5])\ny = 1.0 * (y > 0.5)\nprint(\"Y=\", y[0:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProb y=1 [[0.44132695]\n [0.50520231]\n [0.54533653]\n [0.56898554]\n [0.61125024]]\nY= [[0.]\n [1.]\n [1.]\n [1.]\n [1.]]\n```\n:::\n:::\n\n\nAgora definimos o modelo. A definição é exatamente igual a definição do modelo\nda regressão linear ([exemplo 3](./03-keras.html)) exceto pela funçõ de ativação\n`sigmoid`.\n\nA função de ativação `sigmoid` faz com que output de `np.dot(X, W) + B` que acontece\ndentro da camada densa seja um número entre 0 e 1.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ninput = keras.layers.Input(shape=(2,))\noutput = keras.layers.Dense(units=1, activation=\"sigmoid\")(input)\nmodel = keras.Model(inputs=input, outputs=output)\n\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n input_1 (InputLayer)        [(None, 2)]               0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense (Dense)               (None, 1)                 3         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 3\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 3\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n2022-04-27 15:58:38.446237: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\nAgora vamos *compilar* o modelo passando a função de perda, otimizador e métricas.\nPontos importantes:\n\n1. Como o problema agora é de classificação binária (o `y` pode ter valores 0 ou 1),\nusamos a função de perda `binary_crossentropy`.\n2. Podemos passar uma lista de métricas para o Keras calcular durante o ajuste do modelo.\nPor exemplo aqui, pedimos para ele calcular a acurácia.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nmodel.compile(\n    loss=keras.losses.binary_crossentropy,\n    optimizer=keras.optimizers.SGD(learning_rate=0.1),\n    metrics=[\"accuracy\"]\n)\n```\n:::\n\n\nAgora podemos ajustar o modelo. O parâmetro `validation_split=0.1` fala para o \nKeras separar 10% das observações de `(x,y)` e usá-las apenas para reportar \nmétricas nessa base de validação (as observações são selecionadas de forma aleatória).\n\n::: {.cell .column-screen-right execution_count=6}\n``` {.python .cell-code}\nhistory = model.fit(x=X, y=y, batch_size=32, epochs=20, validation_split=0.1,\n                    verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.5566 - accuracy: 0.8033 - val_loss: 0.4929 - val_accuracy: 0.7700 - 459ms/epoch - 16ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.5110 - accuracy: 0.7011 - val_loss: 0.4615 - val_accuracy: 0.7700 - 46ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.4896 - accuracy: 0.7122 - val_loss: 0.4423 - val_accuracy: 0.7800 - 38ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 4/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.4717 - accuracy: 0.7233 - val_loss: 0.4272 - val_accuracy: 0.8100 - 45ms/epoch - 2ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.4559 - accuracy: 0.7500 - val_loss: 0.4111 - val_accuracy: 0.8200 - 42ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 6/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.4413 - accuracy: 0.7700 - val_loss: 0.3972 - val_accuracy: 0.8400 - 38ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 7/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.4279 - accuracy: 0.7844 - val_loss: 0.3854 - val_accuracy: 0.8400 - 38ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 8/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.4159 - accuracy: 0.8067 - val_loss: 0.3746 - val_accuracy: 0.8700 - 36ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 9/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.4048 - accuracy: 0.8222 - val_loss: 0.3633 - val_accuracy: 0.8800 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 10/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3946 - accuracy: 0.8344 - val_loss: 0.3539 - val_accuracy: 0.8800 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 11/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3851 - accuracy: 0.8444 - val_loss: 0.3443 - val_accuracy: 0.8800 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 12/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3762 - accuracy: 0.8444 - val_loss: 0.3364 - val_accuracy: 0.8900 - 36ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 13/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3679 - accuracy: 0.8567 - val_loss: 0.3293 - val_accuracy: 0.9000 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 14/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3599 - accuracy: 0.8678 - val_loss: 0.3237 - val_accuracy: 0.9200 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 15/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3527 - accuracy: 0.8822 - val_loss: 0.3170 - val_accuracy: 0.9200 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 16/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3457 - accuracy: 0.8833 - val_loss: 0.3108 - val_accuracy: 0.9300 - 38ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 17/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3393 - accuracy: 0.8889 - val_loss: 0.3046 - val_accuracy: 0.9300 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 18/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3331 - accuracy: 0.9044 - val_loss: 0.2972 - val_accuracy: 0.9300 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 19/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3274 - accuracy: 0.8967 - val_loss: 0.2937 - val_accuracy: 0.9600 - 37ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 20/20\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n29/29 - 0s - loss: 0.3219 - accuracy: 0.9089 - val_loss: 0.2866 - val_accuracy: 0.9500 - 39ms/epoch - 1ms/step\n```\n:::\n:::\n\n\nPodemos fazer graáfio da acurácia ao longo das épocas:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsns.lineplot(x=range(20), y=history.history[\"val_accuracy\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<AxesSubplot:>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](05-logistic-regression_files/figure-html/cell-8-output-2.png){width=588 height=404}\n:::\n:::\n\n\nE uma matriz de confusão para o ponto de corte 0.5 - isto é, se a probabilidade\npredita for maior que 0.5 classificamos como `y=1`\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmetrics.confusion_matrix(y, model.predict(X) > 0.5)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([[199,  94],\n       [  0, 707]])\n```\n:::\n:::\n\n\n",
    "supporting": [
      "05-logistic-regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}