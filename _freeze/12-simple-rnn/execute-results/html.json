{
  "hash": "d3912be1c55247cc34e45650efe7bd29",
  "result": {
    "markdown": "---\ntitle: RNN's\n---\n\nNesse exemplo usar RNN's básicas para classificar de sequências simuladas.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom tensorflow import keras\n```\n:::\n\n\nVamos gerar sequências de números aleatórios que podem ser crescentes ou decrescentes:\n\n1. Definimos se a sequência vai ser crescente ou decrescente,\n2. Para cada sequencia, geramos `l` números aleatórios. \n3. Se a sequência for crescente, ordenamos os números de forma crescente, se não for, ordenamos de forma decrescente.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn = 10000\nl = 10\n\ncresc = np.random.randint(0, 2, size=(n,))\nx = np.empty((n, l), dtype=np.float32)\nfor i, cr in enumerate(cresc):\n    tmp = np.random.uniform(size=(l,))\n    if cr == 1:\n        x[i, :] = tmp[np.argsort(tmp)]\n    else:\n        x[i, :] = tmp[np.argsort(-tmp)]\nx = x.reshape((n, l, 1))\n```\n:::\n\n\nAgora vamos definir o modelo no Keras. Não vamos nos preocupar com os\nparâmetros nem nada ainda. A seguir vamos mostrar exatamente as contas que o Keras está fazendo por trás.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ninput = keras.layers.Input(shape=(l, 1))\noutput = keras.layers.SimpleRNN(units=1, activation=\"tanh\", use_bias=False)(input)\noutput = keras.layers.Activation(\"sigmoid\")(output)\nmodel = keras.Model(inputs=input, outputs=output)\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=\"accuracy\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2022-05-01 14:49:29.471829: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\nAgora ajustamos o modelo:\n\n::: {.cell .column-screen-right layout-ncol='1' execution_count=4}\n``` {.python .cell-code}\nmodel.fit(x=x, y=cresc, epochs=10, verbose=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 1s - loss: 0.5114 - accuracy: 0.8961 - 856ms/epoch - 3ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 2/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.4128 - accuracy: 0.9780 - 316ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 3/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.4038 - accuracy: 0.9721 - 308ms/epoch - 984us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 4/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.4014 - accuracy: 0.9684 - 314ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 5/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.4009 - accuracy: 0.9679 - 309ms/epoch - 986us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 6/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.3986 - accuracy: 0.9676 - 311ms/epoch - 992us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 7/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.3972 - accuracy: 0.9698 - 309ms/epoch - 988us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 8/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.3977 - accuracy: 0.9678 - 314ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 9/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.3966 - accuracy: 0.9664 - 313ms/epoch - 1000us/step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 10/10\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n313/313 - 0s - loss: 0.3984 - accuracy: 0.9668 - 316ms/epoch - 1ms/step\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<keras.callbacks.History at 0x7fbe99f1a6a0>\n```\n:::\n:::\n\n\nVimos que a RNN consegue prever bem se a sequência é crescente ou decrescente.\n\nAgora vamos ver exatamente as contas que o Keras faz. \nPrimeiro, definimos a função sigmoid.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n```\n:::\n\n\nDepois, pegamos o valor do peso estimado pelo Keras:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nw_rnn = model.layers[1].get_weights()\n```\n:::\n\n\nVamos escolher a primeira observação, para mostrar a conta:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nx_ = x[0]\n```\n:::\n\n\nAgora o loop que acontece dentro da RNN. Começando com um estado inicial `s=0`, fazemos:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ns = 0.0\nfor i in range(l):\n    s = np.tanh(w_rnn[0] * x_[i] + w_rnn[1] * s)\nsigmoid(s)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([[0.72884184]], dtype=float32)\n```\n:::\n:::\n\n\nEm cada passo o estado é atualizado com um peso para o estado anterior e outro para a nova observação, passando por uma função de ativação.\nPodemos comparar isso com o resultado do Keras:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmodel.predict(x)[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([0.7288419], dtype=float32)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "12-simple-rnn_files"
    ],
    "filters": [],
    "includes": {}
  }
}