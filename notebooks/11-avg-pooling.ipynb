{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Pooling\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesse exemplo vamos usar as camadas de TextVectorization do [exemplo 09](./09-text-vectorization.qmd) e a camada de embedding do [exemplo anterior](./10-embedding.qmd) para transformar cada palavra em um vetor com 1 elemento.\n",
        "\n",
        "Em seguida vamos considerar que cada texto (no caso do banco de dados, comentários feiros em forum da internet) pode ser representado pela média das palavras que têm nele. Usar a média é também chamado de `GlobalAveragePooling` no Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos carregar o banco de dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/deep-learning-com-r/toxic-comments.csv\"\n",
        ")\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nosso objetivo será, a partir do texto do comentário, classificar em tóxico ou não tóxico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = dataset['comment_text'].to_numpy()\n",
        "y = dataset.iloc[:,2].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para escolher o valor de `output_sequence_length`, isto é, o número máximo de palavras em cada texto, fazemos um histograma do número de palavras por texto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "numero_palavras = [len(i.split()) for i in x]\n",
        "sns.histplot(numero_palavras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como a maioria dos textos possui menos do que 150 palavras, escolhemos que o tamanho máximo é 150. Dessa forma, se um texto for muito grande, usamos apenas as primeiras 150 palavras para classificar em tóxico/não tóxico. Isso vai permitir que o modelo fique muito mais rápido.\n",
        "\n",
        "Vamos então criar a camada de vetorização e adaptá-la:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectorize = keras.layers.TextVectorization(\n",
        "    max_tokens=10000, output_mode=\"int\", output_sequence_length=150\n",
        ")\n",
        "\n",
        "vectorize.adapt(x)\n",
        "vocab = vectorize.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos definir o modelo. Lembre que cada palavra será representada por um vetor pela camada de embedding e queremos que a média desses valores represente a probabilidade do texto ser tóxicou ou não."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "input = keras.layers.Input(shape=(), dtype=\"string\")\n",
        "output = vectorize(input)\n",
        "output = keras.layers.Embedding(input_dim=len(vocab), output_dim=1)(output)\n",
        "output = keras.layers.GlobalAveragePooling1D()(output)\n",
        "output = keras.layers.Activation(\"sigmoid\")(output)\n",
        "\n",
        "model = keras.Model(inputs=input, outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesse modelo, o número de parâmetros é igual ao número de palavras que temos no vocabulário.\n",
        "\n",
        "Agora podemos compilar o modelo. Vamos usar a 'binary_crossentropy' pois o problema é de classificação binária. Além da acurácia, vamos medir o AUC, uma vez que o problema é bem desbalanceado (muitos zeros e poucos uns)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "auc = keras.metrics.AUC(curve=\"ROC\")\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", auc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos ajustar o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.fit(x, y, epochs=5, batch_size=32, validation_split=0.2, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos indetificar as palavras com os maiores e menores valores de embedding. Como o texto é classificado em tóxico/não tóxico com base na média dos valores das palavras dele, palavras com valores altos ajudam a classificar o texto como tóxico e palavras com valores baixos ajudam a considerar o texto como não tóxico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "embeddings = model.get_weights()[1]\n",
        "words = pd.DataFrame.from_dict({\n",
        "  \"word\": vocab,\n",
        "  \"embedding\": embeddings[:,0]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As 10 palavras com maires valores de embeddings são:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "words.sort_values(\"embedding\", ascending = False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E as 10 palavras com menores valores são:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "words.sort_values(\"embedding\", ascending = True).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que os embeddings que são comportaram como imaginávamos. Maiores valores indicam palavras que levam à textos tóxicos e valores menores levam a textos não tóxicos."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}