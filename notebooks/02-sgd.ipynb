{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: SGD\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-note}\n",
        "Esse exemplo é muito parecido com o [exemplo 1](./01-linear-regression.html) com apenas modificações no loop de ajuste do modelo para usar o método da descida do gradiente estocástico - *stochastic gradient descent* ou também **SGD**.\n",
        ":::\n",
        "\n",
        "O objetivo desse exemplo é simular alguns dados e ajustar um modelo\n",
        "de regressão linear usando o método **estocástico** da descida do gradiente.\n",
        "Importante notar que queremos implementar o método da descida do gradiente de forma manual, isto é, sem usar bibliotecas que calculam \n",
        "a derivada automaticamente.\n",
        "\n",
        ":::{.callout-note}\n",
        "A vantagem do SGD com relação ao GD é o custom computacional. O primeiro precisa\n",
        "calcular derivadas de todas as observações, calcular o valor predito para todas as\n",
        "observações, etc para cada passo. No **SGD**, como fazemos uma observação de cada vez,\n",
        "chegamos mais rápido no mínimo.\n",
        "\n",
        "Outra vantagem é o uso da memória. Em deep learning muitas vezes não conseguimos\n",
        "carregar todos os dados de uma vez para a memória do computador. No caso do SGD,\n",
        "é trivial implementá-lo sem precisar de todas as observações de uma vez.\n",
        "\n",
        "Além disso, é estudado que o SGD tem um efeito de regularização no ajuste do modelo.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em primeiro lugar simulamos alguns dados. `x` vai ser uma variável contendo `n` valores aleatórios obtidos a partir da distribuição uniforme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n = 1000\n",
        "x = np.random.uniform(size=(n,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em seguida definimos os pesos `W` e `b` para a simulação. Nosso objetivo depois, vai ser *fingir* que não sabemos esses valores e ajustar um modelo para encontrá-los."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "W = 0.9\n",
        "b = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos então a variável resposta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = W * x + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos definir o nosso modelo. Ele é uma classe do python que recebe os valores iniciais dos pesos `w` e `b` na inicialização e possui o método `predict` que\n",
        "faz uma transformação linear `w*x + b` no input `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Model:\n",
        "    def __init__(self, w, b):\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.w * x + self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos definir a função de perda. Nesse caso estamos usando o erro quadrático médio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def loss(y_true, y_hat):\n",
        "    return np.mean((y_true - y_hat) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Também definimos as derivadas da função de perda `l` em relação a cada uma das partes do grafo de computação até chegar nos parâmetros que queremos estimar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# derivada da função de perda em relação à y_hat\n",
        "def dl_dyhat(y_hat, x, y):\n",
        "    return 2 * (y - y_hat) * (-1)\n",
        "\n",
        "# derivada de y_hat com relação a w\n",
        "def dyhat_dw(y_hat, x, y):\n",
        "    return x\n",
        "\n",
        "# derivada de y_hat com relação a b (viés)\n",
        "def dyhat_db(y_hat, x, y):\n",
        "    return 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos inicializar o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "w = np.random.uniform(size=1)\n",
        "b = 0.0\n",
        "model = Model(w, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "E podemos, finalmente, implementar o método da descida do gradiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lr = 0.1 # learning rate - qual o tamanho do passo que damos em cada iteração.\n",
        "num_epochs = 2 # o número de épocas é o número de vezes que passamos pela base inteira.\n",
        "index = np.arange(x.size)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    np.random.shuffle(index)\n",
        "    for i in list(index):\n",
        "        y_hat = model.predict(x[i])\n",
        "        model.w -= lr * np.mean(\n",
        "            dl_dyhat(y_hat, x[i], y[i]) * dyhat_dw(y_hat, x[i], y[i])\n",
        "        )\n",
        "        model.b -= lr * np.mean(\n",
        "            dl_dyhat(y_hat, x[i], y[i]) * dyhat_db(y_hat, x[i], y[i])\n",
        "        )\n",
        "    print(\"epoch:\", epoch, \"loss:\", loss(y, model.predict(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verificamos que o resultado está conforme o esperado. Isto é, `w` e `b` estão\n",
        "parecidos com os valores usamos para gerar os dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.w)\n",
        "print(model.b)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}