{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Aplicando a LSTM\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nesse exemplo vamos aplicar a LSTM ao mesmo banco de dados que fizemos o Pooling. Diferentemente do [exemplo](./11-avg-pooling.qmd) vamos prever para todas as tags do banco de dados. **Note** que são tags, não categorias, cada texto pode ser classificado em mais de uma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos carregar o banco de dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/deep-learning-com-r/toxic-comments.csv\"\n",
        ")\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nosso objetivo será, a partir do texto do comentário, classificar nas diversas tags possíveis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = dataset['comment_text'].to_numpy()\n",
        "y = dataset.iloc[:,2:].to_numpy()\n",
        "print(dataset.columns[2:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos então criar a camada de vetorização e adaptá-la:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectorize = keras.layers.TextVectorization(\n",
        "    max_tokens=10000, output_mode=\"int\", output_sequence_length=150\n",
        ")\n",
        "\n",
        "vectorize.adapt(x)\n",
        "vocab = vectorize.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Em seguida podemos criar o modelo no Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "input = keras.layers.Input(shape=(), dtype=\"string\")\n",
        "output = vectorize(input)\n",
        "output = keras.layers.Embedding(input_dim=len(vocab), output_dim=2)(output)\n",
        "output = keras.layers.LSTM(units=256)(output)\n",
        "output = keras.layers.Dense(units=y.shape[1], activation=\"sigmoid\")(output)\n",
        "\n",
        "model = keras.Model(inputs=input, outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note que na última camada densa temos `units=y.shape[1]`, isto é, um output para cada tag possível. A ativação é sigmoid, pois queremos prever uma probabilidade para cada coluna.\n",
        "\n",
        "Agora vamos compilar e ajustar o modelo;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| layout-ncol: 1\n",
        "#| column: screen-right\n",
        "auc = keras.metrics.AUC(curve=\"ROC\")\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\", auc])\n",
        "\n",
        "model.fit(x, y, epochs=5, batch_size=32, validation_split=0.2, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}